{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad63a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.0+cu117\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1373976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "from mini_avalam import *\n",
    "from Self_Play_mini_avalam import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actions_size = 3*3*8 \n",
    "board_size = 3*3 \n",
    "\n",
    "## create dictionary to map actions from indices to action tuple type\n",
    "def create_action_dictionary():\n",
    "    action_dict = {}\n",
    "    index = 0\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            for drow in range(-1, 2):\n",
    "                for dcol in range(-1, 2):\n",
    "                    if drow == 0 and dcol == 0:\n",
    "                        continue\n",
    "                    new_row = row + drow\n",
    "                    new_col = col + dcol\n",
    "                    if 0 <= new_row < 3 and 0 <= new_col < 3:\n",
    "                        action_dict[(row, col, new_row, new_col)] = index\n",
    "                        index += 1\n",
    "    return action_dict\n",
    "\n",
    "action_dict = create_action_dictionary()\n",
    "index_to_action = {index: action for action, index in action_dict.items()}\n",
    "\n",
    "initial_state = [     [ 1, -1,  1],\n",
    "                      [-1,  1, -1],\n",
    "                      [ 0, -1,  1],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea7ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_actions(top_actions, action_probs):\n",
    "    \"\"\"\n",
    "    Plot the top action probabilities from the MCTS search.\n",
    "\n",
    "    Args:\n",
    "        top_actions (list): A list of action indices representing the top actions.\n",
    "        action_probs (np.array): A numpy array containing the probabilities of each action.\n",
    "    \"\"\"\n",
    "    top_probabilities = [action_probs[action_index] for action_index in top_actions]\n",
    "\n",
    "    # Get the action coordinates from the action indices\n",
    "    top_action_coordinates = [index_to_action[action_index] for action_index in top_actions]\n",
    "\n",
    "    # Create the bar plot with top actions\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(range(len(top_actions)), top_probabilities)\n",
    "    plt.xticks(range(len(top_actions)), top_action_coordinates, rotation=90)\n",
    "    plt.xlabel('Top Actions (from_x, from_y, to_x, to_y)')\n",
    "    plt.ylabel('Policy Probability')\n",
    "    plt.title('Top Policy Probabilities')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c228bf",
   "metadata": {},
   "source": [
    "### Load Model and check the action probabilites and value for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed5a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16817176342010498\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## state Encoding \n",
    "state = np.array([[1, -1,  0],\n",
    "                  [0, -2,  0],\n",
    "                  [0,  3,  0] ])\n",
    "\n",
    "encoded_state = get_encoded_state_(state)\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "## load Model and generate action probabilities and value state\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "#### adjust policy to only valid moves\n",
    "valid_moves = np.zeros_like(policy)\n",
    "for action_index in get_actions_indices_array(state, action_dict):\n",
    "    valid_moves[action_index] = 1.0\n",
    "\n",
    "policy *= valid_moves \n",
    "policy /= np.sum(policy)\n",
    "\n",
    "## Plot action space\n",
    "# Sort actions by their probabilities\n",
    "sorted_actions = np.argsort(policy)[::-1]\n",
    "\n",
    "# Select top 10 actions\n",
    "top_actions = sorted_actions[:10]\n",
    "\n",
    "# Plot the top 10 actions\n",
    "plot_top_actions(top_actions, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf9953",
   "metadata": {},
   "source": [
    "### MCTS and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcee1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS() : \n",
    "\n",
    "    def __init__(self, model, args, device) :\n",
    "        self.args = args\n",
    "        self.model = model\n",
    " \n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "#         define root \n",
    "        root = Node(self.args, state, visit_count=1)  ## board and state mean same thing \n",
    "        \n",
    "        ## add noise \n",
    "        policy, _ = self.model(\n",
    "                    torch.tensor(get_encoded_state_(state)).unsqueeze(0).to(device)\n",
    "                )\n",
    "\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * action_size)\n",
    "                \n",
    "        valid_moves = np.zeros_like(policy)\n",
    "        for action_index in get_actions_indices_array(state, action_dict):\n",
    "            valid_moves[action_index] = 1.0\n",
    "        \n",
    "        policy *= valid_moves \n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            ## Selection \n",
    "            node = root\n",
    "\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            value, is_terminal = -get_score_array(node.board), is_finished_array(node.board)\n",
    "\n",
    "            if not is_terminal: \n",
    "         \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(get_encoded_state_(node.board)).unsqueeze(0).to(device)\n",
    "                ) \n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = np.zeros_like(policy)\n",
    "                for action_index in get_actions_indices_array(node.board,action_dict):\n",
    "                    valid_moves[action_index] = 1.0\n",
    "\n",
    "                policy *= valid_moves \n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "\n",
    "                ## Expansion\n",
    "                node = node.expand(policy)\n",
    "                \n",
    "            ## Backpropagation\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        ## return visit counts \n",
    "        action_probs = [0] * action_size\n",
    "\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        total_visit_count = sum(action_probs)\n",
    "        action_probs = np.array([prob / total_visit_count for prob in action_probs])\n",
    "        return action_probs, root\n",
    "\n",
    "class Node: \n",
    "    def __init__(self, args, board, parent=None, action_taken=None ,prior=0, visit_count=0):\n",
    "        self.args = args \n",
    "        self.board = board \n",
    "        self.parent = parent \n",
    "        self.action_taken = action_taken \n",
    "        self.prior = prior  \n",
    "\n",
    "        self.children = []\n",
    "        self.expandable_moves = list(get_actions_indices_array(board, action_dict))\n",
    "\n",
    "        self.visit_count = 0 \n",
    "        self.value_sum = 0 \n",
    "\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return  len(self.children)>0 \n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None \n",
    "        best_ucb = -np.inf \n",
    "\n",
    "        for child in self.children : \n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb :\n",
    "                best_child = child \n",
    "                best_ucb = ucb \n",
    "\n",
    "        return best_child \n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        prior_score = child.prior * math.sqrt(self.visit_count) / (child.visit_count + 1)\n",
    "        if child.visit_count > 0:\n",
    "            value_score = child.value_sum / child.visit_count\n",
    "        else:\n",
    "            value_score = 0\n",
    "        return value_score + prior_score\n",
    "\n",
    "    def expand(self, policy):\n",
    "\n",
    "        for action_ind, prob in enumerate(policy):\n",
    "            if prob >0 : \n",
    "                \n",
    "                ## Store position of cells and their current height that will be impacted by the move , so we can undo the move \n",
    "                store_move = []\n",
    "                action = index_to_action[action_ind]\n",
    "                store_move.append((action[0] , action[1] , self.board[action[0]][action[1]])) \n",
    "                store_move.append((action[2] , action[3] , self.board[action[2]][action[3]]))\n",
    "\n",
    "                ## play move \n",
    "                play_action_array(self.board,action)  \n",
    "\n",
    "                ## change perspective \n",
    "                self.board = -1 * np.array(self.board)\n",
    "                child_board = np.copy(self.board)\n",
    "                child = Node(self.args, child_board, self, action_ind, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "                ## Undo the move is restoring the cells that were changed by their old state \n",
    "                self.board = -1 * self.board\n",
    "                self.board[store_move[0][0]][store_move[0][1]] = store_move[0][2]\n",
    "                self.board[store_move[1][0]][store_move[1][1]] = store_move[1][2]\n",
    "                \n",
    "        return child\n",
    "\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = -value \n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438bde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best action index: 9\n",
      "Best action: (0, 2, 1, 1)\n",
      "Value : 0.16817176342010498\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = np.array([[1,  1,  1],\n",
    "                  [0, -2, -1],\n",
    "                  [0,  3,  0] ])\n",
    "\n",
    "# state = np.array(initial_state)\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 200,\n",
    "    'action_size': 3 * 3 * 8,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "mcts = MCTS(model, args, device)\n",
    "\n",
    "action_probs, root = mcts.search(state)\n",
    "\n",
    "best_action_index = np.argmax(action_probs)\n",
    "print(\"Best action index:\", best_action_index)\n",
    "best_action = index_to_action[best_action_index]\n",
    "print(\"Best action:\", best_action)\n",
    "print(\"Value :\", value)\n",
    "\n",
    "# Sort actions by their probabilities\n",
    "sorted_actions = np.argsort(action_probs)[::-1]\n",
    "\n",
    "# Select top 10 actions\n",
    "top_actions = sorted_actions[:10]\n",
    "\n",
    "# Plot the top 10 actions\n",
    "plot_top_actions(top_actions, action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4d4c1",
   "metadata": {},
   "source": [
    "### Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01dadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = [     [ 1, -1,  1],\n",
    "                      [-1,  1, -1],\n",
    "                      [ 0, -1,  1],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7656755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero():\n",
    "    def __init__(self, model, optimizer, args):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer \n",
    "        self.args = args \n",
    "        self.mcts = MCTS(model, args, device)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1 \n",
    "        state = np.copy(initial_state) ## initialize game state \n",
    "\n",
    "        while True :       \n",
    "            action_probs, root = self.mcts.search(np.copy(state))\n",
    "            \n",
    "            memory.append((get_encoded_state_(state), action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = np.array(action_probs) ** (1/self.args[\"temperature\"])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action_index = np.random.choice(action_size, p=temperature_action_probs)  ## this action is scalar \n",
    "            action = index_to_action[action_index]\n",
    "            \n",
    "            ## get to next state using action \n",
    "            play_action_array(state,action) \n",
    "\n",
    "            value, is_terminal = get_score_array(state), is_finished_array(state)\n",
    "\n",
    "            if is_terminal : \n",
    "                returnMemory = []\n",
    "                for hist_neutral_state_encoded, hist_action_probs, hist_player in memory : \n",
    "                    hist_outcome = value * hist_player \n",
    "                    returnMemory.append((\n",
    "                        hist_neutral_state_encoded, \n",
    "                        hist_action_probs, \n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            ## change perspective \n",
    "            state = -1 * np.array(state)\n",
    "            player *= -1\n",
    "            \n",
    "            \n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        for iteration in range(self.args[\"num_iterations\"]):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            ## machine plays with itself \n",
    "            for selfPlay_iteration in trange(self.args[\"num_selfPlay_iterations\"]):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            ## train based on the memory collected\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args[\"num_epochs\"]):\n",
    "                self.train(memory)\n",
    "                \n",
    "            iteration += self.args[\"start_iteration\"]\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\" )\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n",
    "            return memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "58fcae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029995441436767578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e8edf57ae94ba9ba5f176e9324b13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.033066511154174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefccb298c504b7792c2c6ac761cae54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 125.44 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_1.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer.load_state_dict(torch.load('optimizer_1.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iterations': 3,\n",
    "    'start_iteration': 2,\n",
    "    'num_parallel_games': 10,\n",
    "    'num_selfPlay_iterations': 100,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory_ = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3fb7e",
   "metadata": {},
   "source": [
    "This part is to check the memory and if all is good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e18055fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchIdx in range(0, len(memory), args['batch_size']):\n",
    "sample = memory_\n",
    "states, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "states, policy_targets, value_targets = np.array(states), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "abcd7856",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0 : score is : -0.2 number of moves : 28\n",
      "[[ 0  1 -1]\n",
      " [-2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 1 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 2 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 3 : score is : 0 number of moves : 20\n",
      "[[ 1  2  0]\n",
      " [-1  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 4 : score is : -0.2 number of moves : 4\n",
      "[[ 0  0 -3]\n",
      " [ 2  0  0]\n",
      " [ 0 -1  2]]\n",
      "state 5 : score is : -0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [-2  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 6 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 7 : score is : 0.2 number of moves : 26\n",
      "[[ 2  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 8 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 9 : score is : -0.4 number of moves : 2\n",
      "[[ 2 -3  0]\n",
      " [ 0  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 10 : score is : -0.4 number of moves : 2\n",
      "[[ 1  0 -3]\n",
      " [-1  0  0]\n",
      " [ 0  0 -3]]\n",
      "state 11 : score is : -0.4 number of moves : 10\n",
      "[[-3  0  1]\n",
      " [-1  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 12 : score is : 0.2 number of moves : 4\n",
      "[[-1  0  0]\n",
      " [ 1 -3  1]\n",
      " [ 0  0  2]]\n",
      "state 13 : score is : -0.4 number of moves : 10\n",
      "[[ 1 -1  1]\n",
      " [-1  0  0]\n",
      " [ 0 -1 -3]]\n",
      "state 14 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 15 : score is : -0.2 number of moves : 14\n",
      "[[ 1 -1 -3]\n",
      " [-1  1  0]\n",
      " [ 0  0  1]]\n",
      "state 16 : score is : -0.4 number of moves : 18\n",
      "[[ 1 -1  0]\n",
      " [-1  2  0]\n",
      " [ 0 -1 -2]]\n",
      "state 17 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 18 : score is : 0.2 number of moves : 28\n",
      "[[ 0  1 -1]\n",
      " [ 1 -2  1]\n",
      " [ 0  1 -1]]\n",
      "state 19 : score is : -0.4 number of moves : 10\n",
      "[[-3  0  1]\n",
      " [-1  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 20 : score is : 0 number of moves : 16\n",
      "[[ 1 -1 -2]\n",
      " [ 0  1  0]\n",
      " [ 0 -2  1]]\n",
      "state 21 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 22 : score is : -0.2 number of moves : 14\n",
      "[[ 1 -3  0]\n",
      " [ 0  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 23 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [-3  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 24 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 25 : score is : 0.2 number of moves : 24\n",
      "[[-1  1 -1]\n",
      " [ 1  2  0]\n",
      " [ 0  1 -1]]\n",
      "state 26 : score is : 0 number of moves : 20\n",
      "[[-2 -1  1]\n",
      " [ 0  1  2]\n",
      " [ 0 -1  0]]\n",
      "state 27 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 28 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 29 : score is : -0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [ 1  0 -2]\n",
      " [ 0  1 -1]]\n",
      "state 30 : score is : -0.2 number of moves : 4\n",
      "[[-1  1 -1]\n",
      " [ 0  0  0]\n",
      " [ 0 -3  2]]\n",
      "state 31 : score is : 0 number of moves : 20\n",
      "[[ 1  2  0]\n",
      " [-1  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 32 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 33 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 34 : score is : -0.4 number of moves : 2\n",
      "[[ 2 -3  0]\n",
      " [ 0  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 35 : score is : -0.2 number of moves : 26\n",
      "[[-1  2 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 36 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  0 -1]]\n",
      "state 37 : score is : -0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0 -2 -1]]\n",
      "state 38 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 2 -3  1]\n",
      " [ 0  0  0]]\n",
      "state 39 : score is : -0.2 number of moves : 10\n",
      "[[ 0  0  3]\n",
      " [-2 -1  0]\n",
      " [ 0  1 -1]]\n",
      "state 40 : score is : -0.2 number of moves : 8\n",
      "[[ 0  1 -1]\n",
      " [-2  2  0]\n",
      " [ 0 -2  0]]\n",
      "state 41 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 42 : score is : 0.2 number of moves : 2\n",
      "[[ 0  0  0]\n",
      " [ 2  3 -1]\n",
      " [ 0  0 -2]]\n",
      "state 43 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 44 : score is : 0.2 number of moves : 8\n",
      "[[ 2  0 -1]\n",
      " [ 1 -1  3]\n",
      " [ 0  0  0]]\n",
      "state 45 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  1 -1]]\n",
      "state 46 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 47 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 48 : score is : 0.2 number of moves : 24\n",
      "[[ 2  0 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 49 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -3  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 50 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 51 : score is : -0.2 number of moves : 24\n",
      "[[-1  2 -1]\n",
      " [ 1 -1  0]\n",
      " [ 0  1 -1]]\n",
      "state 52 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 53 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -3  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 54 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 55 : score is : 0.2 number of moves : 6\n",
      "[[ 0 -2  2]\n",
      " [ 0  2  0]\n",
      " [ 0  1 -1]]\n",
      "state 56 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 57 : score is : 0 number of moves : 16\n",
      "[[-2  0  1]\n",
      " [-1  1 -2]\n",
      " [ 0  0  1]]\n",
      "state 58 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 59 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 0  3  2]\n",
      " [ 0  0 -1]]\n",
      "state 60 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 61 : score is : 0.4 number of moves : 2\n",
      "[[-3  0  1]\n",
      " [ 0  1  0]\n",
      " [ 0  3  0]]\n",
      "state 62 : score is : -0.2 number of moves : 28\n",
      "[[-1  1 -1]\n",
      " [ 1 -1 -2]\n",
      " [ 0  1  0]]\n",
      "state 63 : score is : 0.2 number of moves : 8\n",
      "[[ 0  1 -2]\n",
      " [-2  0  0]\n",
      " [ 0  1  2]]\n",
      "state 64 : score is : -0.4 number of moves : 2\n",
      "[[-2 -1  0]\n",
      " [ 0  3  0]\n",
      " [ 0  0 -2]]\n",
      "state 65 : score is : 0.2 number of moves : 4\n",
      "[[-1  1  2]\n",
      " [-3  0  0]\n",
      " [ 0  1  0]]\n",
      "state 66 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 67 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 68 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 69 : score is : -0.2 number of moves : 8\n",
      "[[-1 -2  0]\n",
      " [ 1  0  0]\n",
      " [ 0  1 -3]]\n",
      "state 70 : score is : 0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 71 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  0  2]]\n",
      "state 72 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 73 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 74 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 75 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 76 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 77 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 78 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 79 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 80 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 81 : score is : -0.4 number of moves : 2\n",
      "[[ 1 -3  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -3]]\n",
      "state 82 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 83 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 84 : score is : 0.2 number of moves : 20\n",
      "[[-2  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 85 : score is : 0.4 number of moves : 2\n",
      "[[ 0  2  0]\n",
      " [-3  0  2]\n",
      " [ 0  0  1]]\n",
      "state 86 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 87 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 88 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 89 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  0 -1]]\n",
      "state 90 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 91 : score is : -0.2 number of moves : 6\n",
      "[[-1  2 -1]\n",
      " [ 2  0  0]\n",
      " [ 0  0 -2]]\n",
      "state 92 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 93 : score is : 0 number of moves : 18\n",
      "[[ 0 -1  1]\n",
      " [ 2  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 94 : score is : -0.2 number of moves : 12\n",
      "[[-3  0  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 95 : score is : -0.2 number of moves : 14\n",
      "[[ 1 -1 -3]\n",
      " [-1  1  0]\n",
      " [ 0  0  1]]\n",
      "state 96 : score is : -0.4 number of moves : 16\n",
      "[[-2 -1  1]\n",
      " [ 0  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 97 : score is : -0.4 number of moves : 4\n",
      "[[ 0  0 -2]\n",
      " [-3  1  0]\n",
      " [ 0  0 -2]]\n",
      "state 98 : score is : -0.2 number of moves : 2\n",
      "[[ 0  0  1]\n",
      " [-3 -2  0]\n",
      " [ 0  2  0]]\n",
      "state 99 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 100 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 101 : score is : -0.4 number of moves : 2\n",
      "[[ 1 -3  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -3]]\n",
      "state 102 : score is : -0.8 number of moves : 2\n",
      "[[-2 -1  0]\n",
      " [ 0 -3  0]\n",
      " [ 0  0 -2]]\n",
      "state 103 : score is : 0.2 number of moves : 8\n",
      "[[-1  0  3]\n",
      " [ 1  2  0]\n",
      " [ 0  0 -1]]\n",
      "state 104 : score is : -0.2 number of moves : 2\n",
      "[[-1  0 -1]\n",
      " [-3  0  0]\n",
      " [ 0  1  2]]\n",
      "state 105 : score is : 0.2 number of moves : 4\n",
      "[[-1  0  2]\n",
      " [ 0  3  1]\n",
      " [ 0  0 -1]]\n",
      "state 106 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 107 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  0 -1]]\n",
      "state 108 : score is : 0.2 number of moves : 6\n",
      "[[-1  1  0]\n",
      " [ 1 -3  0]\n",
      " [ 0  0  2]]\n",
      "state 109 : score is : -0.2 number of moves : 6\n",
      "[[-1 -2  0]\n",
      " [ 1  3  0]\n",
      " [ 0  0 -1]]\n",
      "state 110 : score is : 0.2 number of moves : 2\n",
      "[[-1  3  2]\n",
      " [ 0  0  0]\n",
      " [ 0  1 -1]]\n",
      "state 111 : score is : -0.2 number of moves : 14\n",
      "[[ 1  0 -3]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 112 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 113 : score is : 0.2 number of moves : 4\n",
      "[[-1  1  2]\n",
      " [ 0  0  0]\n",
      " [ 0  3 -1]]\n",
      "state 114 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 115 : score is : -0.4 number of moves : 2\n",
      "[[ 0 -1 -2]\n",
      " [ 0  3  0]\n",
      " [ 0  0 -2]]\n",
      "state 116 : score is : 0.4 number of moves : 18\n",
      "[[ 1 -1  0]\n",
      " [ 0  1  2]\n",
      " [ 0 -2  1]]\n",
      "state 117 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 118 : score is : 0.2 number of moves : 20\n",
      "[[-2  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 119 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 120 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 121 : score is : 0.2 number of moves : 6\n",
      "[[-1  1  0]\n",
      " [ 1 -3  0]\n",
      " [ 0  0  2]]\n",
      "state 122 : score is : 0.2 number of moves : 4\n",
      "[[ 0 -3 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  0  2]]\n",
      "state 123 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 124 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 125 : score is : 0.2 number of moves : 28\n",
      "[[-1  1  0]\n",
      " [ 1 -2  1]\n",
      " [ 0  1 -1]]\n",
      "state 126 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 127 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 128 : score is : -0.2 number of moves : 14\n",
      "[[ 1  0 -3]\n",
      " [ 0  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 129 : score is : -0.4 number of moves : 2\n",
      "[[-2  0  0]\n",
      " [-1  3  0]\n",
      " [ 0  0 -2]]\n",
      "state 130 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  0 -1]]\n",
      "state 131 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 132 : score is : 0 number of moves : 14\n",
      "[[ 1 -1  1]\n",
      " [ 0  1  0]\n",
      " [ 0 -2 -2]]\n",
      "state 133 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 134 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 135 : score is : -0.2 number of moves : 2\n",
      "[[-2  0  1]\n",
      " [ 0  0  2]\n",
      " [ 0  0 -3]]\n",
      "state 136 : score is : 0.2 number of moves : 10\n",
      "[[-1  1  2]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  3]]\n",
      "state 137 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -3  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 138 : score is : 0.2 number of moves : 20\n",
      "[[-2  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 139 : score is : -0.2 number of moves : 4\n",
      "[[ 1  2  0]\n",
      " [ 0 -2  0]\n",
      " [ 0  0 -3]]\n",
      "state 140 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 141 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 142 : score is : 0 number of moves : 14\n",
      "[[ 2 -1  1]\n",
      " [-1  0 -2]\n",
      " [ 0  0  1]]\n",
      "state 143 : score is : -0.4 number of moves : 2\n",
      "[[ 1  0 -3]\n",
      " [ 0 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 144 : score is : -0.2 number of moves : 14\n",
      "[[ 1 -3  0]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 145 : score is : -0.4 number of moves : 2\n",
      "[[ 0 -1 -2]\n",
      " [ 0  3  0]\n",
      " [ 0  0 -2]]\n",
      "state 146 : score is : -0.2 number of moves : 10\n",
      "[[-1 -2  0]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 147 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 148 : score is : 0.2 number of moves : 6\n",
      "[[-2  3  0]\n",
      " [ 0  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 149 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 150 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 151 : score is : 0.2 number of moves : 4\n",
      "[[ 0  1  2]\n",
      " [ 0 -3  0]\n",
      " [ 0  1 -1]]\n",
      "state 152 : score is : 0.2 number of moves : 2\n",
      "[[-1  0 -1]\n",
      " [ 0  3  0]\n",
      " [ 0  1  2]]\n",
      "state 153 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 154 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 155 : score is : -0.2 number of moves : 8\n",
      "[[ 1 -1  1]\n",
      " [-1 -3  0]\n",
      " [ 0  0  1]]\n",
      "state 156 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 157 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 158 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 159 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 160 : score is : 0.2 number of moves : 4\n",
      "[[ 0  3 -2]\n",
      " [ 1  0  0]\n",
      " [ 0  1 -1]]\n",
      "state 161 : score is : 0 number of moves : 16\n",
      "[[ 0 -1  1]\n",
      " [ 2 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 162 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 163 : score is : -0.6 number of moves : 4\n",
      "[[-1 -2 -1]\n",
      " [ 0  0  3]\n",
      " [ 0  0 -1]]\n",
      "state 164 : score is : 0.2 number of moves : 28\n",
      "[[-1  1  0]\n",
      " [ 1 -2  1]\n",
      " [ 0  1 -1]]\n",
      "state 165 : score is : 0.4 number of moves : 2\n",
      "[[-2  0  0]\n",
      " [ 0  0  2]\n",
      " [ 0  3  1]]\n",
      "state 166 : score is : -0.2 number of moves : 6\n",
      "[[-1  0  3]\n",
      " [ 0  0  1]\n",
      " [ 0 -2 -1]]\n",
      "state 167 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 168 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 169 : score is : 0 number of moves : 2\n",
      "[[-3  0  1]\n",
      " [ 0  3 -1]\n",
      " [ 0  0  0]]\n",
      "state 170 : score is : -0.2 number of moves : 2\n",
      "[[-1  0 -1]\n",
      " [ 0  3  0]\n",
      " [ 0  2 -1]]\n",
      "state 171 : score is : -0.2 number of moves : 8\n",
      "[[ 0  3 -1]\n",
      " [ 0 -1  0]\n",
      " [ 0  2 -1]]\n",
      "state 172 : score is : 0.2 number of moves : 6\n",
      "[[ 3  0 -1]\n",
      " [ 1  0  1]\n",
      " [ 0 -2  0]]\n",
      "state 173 : score is : 0.6 number of moves : 10\n",
      "[[ 2  0  0]\n",
      " [ 1 -2  0]\n",
      " [ 0  1  2]]\n",
      "state 174 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 175 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 176 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 177 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -3]\n",
      " [ 2  0  1]\n",
      " [ 0  0 -1]]\n",
      "state 178 : score is : 0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -2]]\n",
      "state 179 : score is : -0.2 number of moves : 10\n",
      "[[-1  2 -1]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  3]]\n",
      "state 180 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 181 : score is : 0.2 number of moves : 26\n",
      "[[ 2  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 182 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 183 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 3  0  1]\n",
      " [ 0 -2  0]]\n",
      "state 184 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 185 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 186 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  0 -1]]\n",
      "state 187 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 188 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 189 : score is : -0.2 number of moves : 4\n",
      "[[ 0 -2 -1]\n",
      " [ 1  0  3]\n",
      " [ 0  0 -1]]\n",
      "state 190 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 191 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 192 : score is : -0.2 number of moves : 8\n",
      "[[ 0  0 -1]\n",
      " [-2  0  2]\n",
      " [ 0  1 -2]]\n",
      "state 193 : score is : -0.2 number of moves : 8\n",
      "[[-1 -2 -1]\n",
      " [ 0  0  1]\n",
      " [ 0  0  3]]\n",
      "state 194 : score is : 0 number of moves : 18\n",
      "[[ 0  2  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 195 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 196 : score is : 0.2 number of moves : 28\n",
      "[[-1  1 -1]\n",
      " [ 1 -2  1]\n",
      " [ 0  1  0]]\n",
      "state 197 : score is : -0.4 number of moves : 4\n",
      "[[-2  0 -2]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -3]]\n",
      "state 198 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 199 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 200 : score is : -0.2 number of moves : 8\n",
      "[[-1  1 -1]\n",
      " [ 0  0 -2]\n",
      " [ 0  0  3]]\n",
      "state 201 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 202 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 203 : score is : 0.2 number of moves : 8\n",
      "[[ 0  1 -1]\n",
      " [-2  2  0]\n",
      " [ 0  0  2]]\n",
      "state 204 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 205 : score is : -0.4 number of moves : 2\n",
      "[[ 0 -3 -3]\n",
      " [ 0  1  0]\n",
      " [ 0  0  1]]\n",
      "state 206 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 207 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 208 : score is : -0.6 number of moves : 8\n",
      "[[ 0 -2  0]\n",
      " [-2  0 -2]\n",
      " [ 0  1 -1]]\n",
      "state 209 : score is : -0.2 number of moves : 4\n",
      "[[-1  1  2]\n",
      " [ 0  0  0]\n",
      " [ 0 -3 -1]]\n",
      "state 210 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 211 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 212 : score is : 0.2 number of moves : 2\n",
      "[[ 0  0 -2]\n",
      " [ 3  0 -2]\n",
      " [ 0  0  1]]\n",
      "state 213 : score is : 0 number of moves : 16\n",
      "[[ 1 -1 -2]\n",
      " [ 0  1  0]\n",
      " [ 0 -2  1]]\n",
      "state 214 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 215 : score is : -0.4 number of moves : 2\n",
      "[[ 0  0  2]\n",
      " [-3  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 216 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 217 : score is : 0.2 number of moves : 6\n",
      "[[ 3  0 -1]\n",
      " [ 1  0 -2]\n",
      " [ 0  1  0]]\n",
      "state 218 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 219 : score is : 0.2 number of moves : 20\n",
      "[[-2  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 220 : score is : -0.2 number of moves : 8\n",
      "[[-1  3  0]\n",
      " [ 2 -1  0]\n",
      " [ 0  0 -1]]\n",
      "state 221 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  0 -1]]\n",
      "state 222 : score is : -0.2 number of moves : 2\n",
      "[[ 0 -3  1]\n",
      " [ 0  2  0]\n",
      " [ 0 -2  0]]\n",
      "state 223 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  0 -1]]\n",
      "state 224 : score is : -0.4 number of moves : 18\n",
      "[[ 1 -1  0]\n",
      " [-1  2  0]\n",
      " [ 0 -1 -2]]\n",
      "state 225 : score is : 0.2 number of moves : 2\n",
      "[[-2  3  0]\n",
      " [ 0  0 -2]\n",
      " [ 0  0  1]]\n",
      "state 226 : score is : 0.2 number of moves : 24\n",
      "[[-1  0  2]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 227 : score is : -0.2 number of moves : 14\n",
      "[[ 1 -3  0]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 228 : score is : -0.2 number of moves : 2\n",
      "[[ 2  0 -1]\n",
      " [ 1  0 -3]\n",
      " [ 0  0 -1]]\n",
      "state 229 : score is : -0.8 number of moves : 2\n",
      "[[-2  0  0]\n",
      " [-1 -3  0]\n",
      " [ 0  0 -2]]\n",
      "state 230 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 231 : score is : -0.2 number of moves : 20\n",
      "[[-1 -2 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 232 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 233 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 234 : score is : -0.4 number of moves : 8\n",
      "[[-3 -1  1]\n",
      " [-1  0  0]\n",
      " [ 0 -1  1]]\n",
      "state 235 : score is : 0 number of moves : 2\n",
      "[[-3  0  3]\n",
      " [ 0  0 -1]\n",
      " [ 0  0  1]]\n",
      "state 236 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 237 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 238 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 239 : score is : -0.2 number of moves : 14\n",
      "[[ 1  0 -3]\n",
      " [ 0  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 240 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 241 : score is : -0.4 number of moves : 14\n",
      "[[ 1  0 -2]\n",
      " [-1 -2 -1]\n",
      " [ 0  0  1]]\n",
      "state 242 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 243 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  1 -1]]\n",
      "state 244 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 245 : score is : -0.6 number of moves : 10\n",
      "[[-1  2 -1]\n",
      " [ 0 -1  0]\n",
      " [ 0 -3  0]]\n",
      "state 246 : score is : -0.4 number of moves : 18\n",
      "[[ 1 -1 -2]\n",
      " [-1  1  0]\n",
      " [ 0  0 -2]]\n",
      "state 247 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 248 : score is : -0.4 number of moves : 18\n",
      "[[ 0 -1  1]\n",
      " [-1  2 -1]\n",
      " [ 0  0 -2]]\n",
      "state 249 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 250 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 251 : score is : -0.4 number of moves : 18\n",
      "[[ 1 -1 -2]\n",
      " [-1  2  0]\n",
      " [ 0 -1  0]]\n",
      "state 252 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  0 -1]]\n",
      "state 253 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 0  3  1]\n",
      " [ 0 -2  0]]\n",
      "state 254 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 255 : score is : 0.4 number of moves : 12\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -3]\n",
      " [ 0  0  1]]\n",
      "state 256 : score is : -0.2 number of moves : 10\n",
      "[[-1 -2  0]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 257 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 258 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 259 : score is : 0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -2]]\n",
      "state 260 : score is : 0.2 number of moves : 6\n",
      "[[ 3 -2  0]\n",
      " [ 1  0  0]\n",
      " [ 0  1 -1]]\n",
      "state 261 : score is : 0.4 number of moves : 14\n",
      "[[ 0  2  1]\n",
      " [ 2  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 262 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 263 : score is : 0 number of moves : 2\n",
      "[[ 0  0  3]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 264 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 265 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 266 : score is : 0.2 number of moves : 24\n",
      "[[-1  0  2]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 267 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 268 : score is : -0.2 number of moves : 16\n",
      "[[-3 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  1]]\n",
      "state 269 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 270 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -2 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 271 : score is : 0.2 number of moves : 2\n",
      "[[-1  3 -1]\n",
      " [ 1  0  0]\n",
      " [ 0  0  2]]\n",
      "state 272 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -3  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 273 : score is : -0.2 number of moves : 28\n",
      "[[ 0  1 -1]\n",
      " [-2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 274 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 275 : score is : -0.4 number of moves : 10\n",
      "[[-3  0  1]\n",
      " [-1  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 276 : score is : -0.4 number of moves : 2\n",
      "[[ 0 -3  1]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -3]]\n",
      "state 277 : score is : -0.2 number of moves : 16\n",
      "[[-3 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  1]]\n",
      "state 278 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 279 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 280 : score is : -0.2 number of moves : 2\n",
      "[[-3  0  1]\n",
      " [-2  2  0]\n",
      " [ 0  0  0]]\n",
      "state 281 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -3]\n",
      " [ 0  0  0]]\n",
      "state 282 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 283 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 284 : score is : -0.4 number of moves : 10\n",
      "[[-3  0  1]\n",
      " [-1  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 285 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 286 : score is : -0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [-2  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 287 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [-3  0 -1]\n",
      " [ 0 -1  1]]\n",
      "state 288 : score is : 0.2 number of moves : 10\n",
      "[[-1 -2  0]\n",
      " [ 1  2  0]\n",
      " [ 0  0  2]]\n",
      "state 289 : score is : 0.2 number of moves : 8\n",
      "[[ 3  0  0]\n",
      " [ 1  0 -2]\n",
      " [ 0  1 -1]]\n",
      "state 290 : score is : -0.6 number of moves : 12\n",
      "[[ 0 -2  0]\n",
      " [ 0 -1 -2]\n",
      " [ 0  2 -1]]\n",
      "state 291 : score is : 0.2 number of moves : 10\n",
      "[[ 0  3  0]\n",
      " [ 1 -2  0]\n",
      " [ 0  1 -1]]\n",
      "state 292 : score is : 0.2 number of moves : 4\n",
      "[[-1  0  2]\n",
      " [ 1 -3  1]\n",
      " [ 0  0  0]]\n",
      "state 293 : score is : -0.2 number of moves : 4\n",
      "[[-1  1  2]\n",
      " [ 0  0  0]\n",
      " [ 0 -3 -1]]\n",
      "state 294 : score is : -0.2 number of moves : 8\n",
      "[[ 1 -1  1]\n",
      " [-1  0  0]\n",
      " [ 0 -3  1]]\n",
      "state 295 : score is : -0.4 number of moves : 2\n",
      "[[-2  0 -2]\n",
      " [ 0  3 -1]\n",
      " [ 0  0  0]]\n",
      "state 296 : score is : 0.4 number of moves : 12\n",
      "[[ 1 -1  1]\n",
      " [ 0  1  0]\n",
      " [ 0 -3  1]]\n",
      "state 297 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 298 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 299 : score is : -0.2 number of moves : 26\n",
      "[[-1  2 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 300 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 301 : score is : -0.2 number of moves : 2\n",
      "[[-3  2  1]\n",
      " [ 0  0  0]\n",
      " [ 0  0 -2]]\n",
      "state 302 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 303 : score is : -0.4 number of moves : 2\n",
      "[[ 0  0 -3]\n",
      " [-3  1  0]\n",
      " [ 0  0  1]]\n",
      "state 304 : score is : -0.2 number of moves : 6\n",
      "[[ 0  3 -1]\n",
      " [ 2 -1  0]\n",
      " [ 0  0 -1]]\n",
      "state 305 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 306 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 307 : score is : 0.2 number of moves : 6\n",
      "[[ 0  1 -1]\n",
      " [-2  0  0]\n",
      " [ 0  1  3]]\n",
      "state 308 : score is : 0.2 number of moves : 10\n",
      "[[-1  1  0]\n",
      " [ 0 -2  1]\n",
      " [ 0  0  3]]\n",
      "state 309 : score is : -0.4 number of moves : 2\n",
      "[[ 0  0 -3]\n",
      " [-3  1  0]\n",
      " [ 0  0  1]]\n",
      "state 310 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 311 : score is : 0 number of moves : 20\n",
      "[[ 1  2  0]\n",
      " [-1  1  0]\n",
      " [ 0 -1 -2]]\n",
      "state 312 : score is : 0.2 number of moves : 6\n",
      "[[ 0 -2  2]\n",
      " [ 0  2  0]\n",
      " [ 0  1 -1]]\n",
      "state 313 : score is : 0.2 number of moves : 4\n",
      "[[ 0  1  2]\n",
      " [ 0 -3  0]\n",
      " [ 0  1 -1]]\n",
      "state 314 : score is : 0.4 number of moves : 20\n",
      "[[ 0  0  1]\n",
      " [ 2  1 -2]\n",
      " [ 0 -1  1]]\n",
      "state 315 : score is : -0.2 number of moves : 8\n",
      "[[ 0  1  3]\n",
      " [-2 -1  0]\n",
      " [ 0  0 -1]]\n",
      "state 316 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 317 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 0  3  1]\n",
      " [ 0 -2  0]]\n",
      "state 318 : score is : 0 number of moves : 18\n",
      "[[ 1 -1  0]\n",
      " [-1  2 -2]\n",
      " [ 0  0  1]]\n",
      "state 319 : score is : -0.2 number of moves : 12\n",
      "[[ 3  1 -1]\n",
      " [ 0 -1 -2]\n",
      " [ 0  0  0]]\n",
      "state 320 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 321 : score is : -0.2 number of moves : 8\n",
      "[[-1 -2  0]\n",
      " [ 1  0  0]\n",
      " [ 0  1 -3]]\n",
      "state 322 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 3  0  2]\n",
      " [ 0  0 -1]]\n",
      "state 323 : score is : -0.2 number of moves : 4\n",
      "[[-1  0 -1]\n",
      " [ 0  3 -2]\n",
      " [ 0  1  0]]\n",
      "state 324 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 325 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 326 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 327 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 328 : score is : 0.2 number of moves : 8\n",
      "[[-2 -2  0]\n",
      " [ 1  0  0]\n",
      " [ 0  1  2]]\n",
      "state 329 : score is : -0.2 number of moves : 10\n",
      "[[-1 -2  0]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 330 : score is : 0 number of moves : 12\n",
      "[[ 1  2  1]\n",
      " [-1  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 331 : score is : 0.4 number of moves : 22\n",
      "[[ 0  2  1]\n",
      " [ 0  1 -1]\n",
      " [ 0 -2  1]]\n",
      "state 332 : score is : 0.6 number of moves : 8\n",
      "[[ 2  1  0]\n",
      " [ 0 -2  0]\n",
      " [ 0  1  2]]\n",
      "state 333 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 334 : score is : -0.2 number of moves : 8\n",
      "[[ 3 -2 -1]\n",
      " [ 0  0  1]\n",
      " [ 0  0 -1]]\n",
      "state 335 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 336 : score is : -0.2 number of moves : 8\n",
      "[[ 0 -2  3]\n",
      " [ 1 -1  0]\n",
      " [ 0  0 -1]]\n",
      "state 337 : score is : -0.2 number of moves : 6\n",
      "[[-1  0  3]\n",
      " [ 0  0  1]\n",
      " [ 0 -2 -1]]\n",
      "state 338 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 339 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 340 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 341 : score is : -0.2 number of moves : 14\n",
      "[[ 1  0 -3]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 342 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 343 : score is : 0 number of moves : 16\n",
      "[[ 1  0 -2]\n",
      " [-2  1 -1]\n",
      " [ 0  0  1]]\n",
      "state 344 : score is : -0.6 number of moves : 4\n",
      "[[-1 -2 -1]\n",
      " [ 0  0  0]\n",
      " [ 0  3 -1]]\n",
      "state 345 : score is : 0.2 number of moves : 8\n",
      "[[ 0  3 -1]\n",
      " [ 1 -2  0]\n",
      " [ 0  1  0]]\n",
      "state 346 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 347 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 348 : score is : 0.2 number of moves : 24\n",
      "[[-1  0  2]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 349 : score is : 0.4 number of moves : 16\n",
      "[[ 1 -2  1]\n",
      " [ 0  1  0]\n",
      " [ 0 -2  1]]\n",
      "state 350 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  1 -1]]\n",
      "state 351 : score is : -0.4 number of moves : 12\n",
      "[[ 1 -1  2]\n",
      " [-1  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 352 : score is : -0.4 number of moves : 14\n",
      "[[ 1  0  1]\n",
      " [-1 -2 -1]\n",
      " [ 0  0 -2]]\n",
      "state 353 : score is : -0.2 number of moves : 8\n",
      "[[ 0  1 -1]\n",
      " [-3  0  2]\n",
      " [ 0  0 -1]]\n",
      "state 354 : score is : 0.2 number of moves : 6\n",
      "[[-1  1  0]\n",
      " [ 1 -3  0]\n",
      " [ 0  0  2]]\n",
      "state 355 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 356 : score is : -0.2 number of moves : 8\n",
      "[[ 1 -1  1]\n",
      " [-1  0 -3]\n",
      " [ 0  0  1]]\n",
      "state 357 : score is : 0.2 number of moves : 8\n",
      "[[-1  0  2]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 358 : score is : 0.2 number of moves : 4\n",
      "[[ 3  0 -1]\n",
      " [ 2  0  1]\n",
      " [ 0  0 -1]]\n",
      "state 359 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 360 : score is : -0.2 number of moves : 28\n",
      "[[ 0  1 -1]\n",
      " [-2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 361 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 362 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 363 : score is : 0.2 number of moves : 2\n",
      "[[ 0  2 -2]\n",
      " [-1  0  0]\n",
      " [ 0  0  3]]\n",
      "state 364 : score is : 0 number of moves : 14\n",
      "[[ 1  0  1]\n",
      " [-2 -2 -1]\n",
      " [ 0  0  1]]\n",
      "state 365 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 366 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  2]\n",
      " [ 0  0 -1]]\n",
      "state 367 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  0 -1]]\n",
      "state 368 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 369 : score is : -0.2 number of moves : 10\n",
      "[[ 2 -2  0]\n",
      " [ 0 -1 -2]\n",
      " [ 0  1  0]]\n",
      "state 370 : score is : -0.4 number of moves : 2\n",
      "[[-2  3  0]\n",
      " [ 0  0  0]\n",
      " [ 0 -1 -2]]\n",
      "state 371 : score is : -0.2 number of moves : 6\n",
      "[[-1 -2  0]\n",
      " [ 1  0  0]\n",
      " [ 0  3 -1]]\n",
      "state 372 : score is : 0.2 number of moves : 20\n",
      "[[-1  1 -2]\n",
      " [ 1  0  1]\n",
      " [ 0  1 -1]]\n",
      "state 373 : score is : 0.2 number of moves : 8\n",
      "[[-1  0  3]\n",
      " [ 0  2  0]\n",
      " [ 0  1 -1]]\n",
      "state 374 : score is : -0.2 number of moves : 2\n",
      "[[ 0  2  1]\n",
      " [ 0 -3  0]\n",
      " [ 0  0 -2]]\n",
      "state 375 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 376 : score is : 0.2 number of moves : 6\n",
      "[[-1 -3  0]\n",
      " [ 1  0  0]\n",
      " [ 0  1  2]]\n",
      "state 377 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 378 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 379 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 380 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 381 : score is : 0.4 number of moves : 4\n",
      "[[ 0  0  0]\n",
      " [ 2  1 -3]\n",
      " [ 0  2  0]]\n",
      "state 382 : score is : -0.2 number of moves : 28\n",
      "[[-1 -2  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 383 : score is : 0.2 number of moves : 8\n",
      "[[-1  1 -1]\n",
      " [ 2  0  0]\n",
      " [ 0  0  3]]\n",
      "state 384 : score is : -0.6 number of moves : 6\n",
      "[[-1  1  0]\n",
      " [ 0  0 -2]\n",
      " [ 0 -3 -1]]\n",
      "state 385 : score is : -0.4 number of moves : 16\n",
      "[[-2  0 -2]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "state 386 : score is : 0.2 number of moves : 24\n",
      "[[-1  1 -1]\n",
      " [ 1 -1  0]\n",
      " [ 0  1  2]]\n",
      "state 387 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 388 : score is : 0.2 number of moves : 8\n",
      "[[ 0  1 -1]\n",
      " [ 3 -1  0]\n",
      " [ 0  0  2]]\n",
      "state 389 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 390 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 391 : score is : -0.6 number of moves : 12\n",
      "[[ 0  1  0]\n",
      " [-2 -1 -2]\n",
      " [ 0 -2  0]]\n",
      "state 392 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 393 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 394 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 395 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 396 : score is : 0.6 number of moves : 4\n",
      "[[ 3  1  2]\n",
      " [ 0  0  0]\n",
      " [ 0  1 -1]]\n",
      "state 397 : score is : -0.4 number of moves : 14\n",
      "[[ 1 -1 -2]\n",
      " [ 0 -2  0]\n",
      " [ 0 -1  1]]\n",
      "state 398 : score is : -0.2 number of moves : 6\n",
      "[[ 3  0 -1]\n",
      " [ 2 -1  0]\n",
      " [ 0  0 -1]]\n",
      "state 399 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 400 : score is : -0.4 number of moves : 2\n",
      "[[-3  0 -3]\n",
      " [ 0  0  0]\n",
      " [ 0 -1  1]]\n",
      "state 401 : score is : -0.4 number of moves : 14\n",
      "[[ 1  0 -2]\n",
      " [-1 -2 -1]\n",
      " [ 0  0  1]]\n",
      "state 402 : score is : -0.4 number of moves : 2\n",
      "[[ 0  0 -3]\n",
      " [-3  1  0]\n",
      " [ 0  0  1]]\n",
      "state 403 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 404 : score is : -0.6 number of moves : 4\n",
      "[[-1 -2 -1]\n",
      " [ 0  0  0]\n",
      " [ 0  3 -1]]\n",
      "state 405 : score is : -0.4 number of moves : 2\n",
      "[[-3  0  1]\n",
      " [ 0  1 -3]\n",
      " [ 0  0  0]]\n",
      "state 406 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 407 : score is : -0.2 number of moves : 24\n",
      "[[-1  0 -1]\n",
      " [ 2 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 408 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 409 : score is : 0.4 number of moves : 2\n",
      "[[ 0  2  0]\n",
      " [ 0 -3  2]\n",
      " [ 0  0  1]]\n",
      "state 410 : score is : 0.4 number of moves : 12\n",
      "[[ 1 -1  1]\n",
      " [ 0  1  0]\n",
      " [ 0 -3  1]]\n",
      "state 411 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0  2  1]\n",
      " [ 0  1 -1]]\n",
      "state 412 : score is : -0.4 number of moves : 10\n",
      "[[ 1 -1  1]\n",
      " [-1  0  0]\n",
      " [ 0 -1 -3]]\n",
      "state 413 : score is : 0.2 number of moves : 8\n",
      "[[-1  1  2]\n",
      " [ 1  0  0]\n",
      " [ 0  0 -3]]\n",
      "state 414 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 415 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 416 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 417 : score is : -0.2 number of moves : 4\n",
      "[[-1 -3  0]\n",
      " [ 1  0  2]\n",
      " [ 0  0 -1]]\n",
      "state 418 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 419 : score is : 0.2 number of moves : 12\n",
      "[[ 2  0  2]\n",
      " [ 1 -1  0]\n",
      " [ 0 -2  0]]\n",
      "state 420 : score is : -0.4 number of moves : 14\n",
      "[[ 1  0 -2]\n",
      " [-1 -2 -1]\n",
      " [ 0  0  1]]\n",
      "state 421 : score is : -0.2 number of moves : 10\n",
      "[[ 0 -2 -1]\n",
      " [ 0 -1  0]\n",
      " [ 0  2  2]]\n",
      "state 422 : score is : -0.2 number of moves : 8\n",
      "[[ 1  0  1]\n",
      " [ 0 -3 -1]\n",
      " [ 0 -1  1]]\n",
      "state 423 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 424 : score is : -0.4 number of moves : 2\n",
      "[[-3  0  1]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -3]]\n",
      "state 425 : score is : -0.2 number of moves : 10\n",
      "[[-1 -2  0]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 426 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 427 : score is : -0.4 number of moves : 2\n",
      "[[-2  0  1]\n",
      " [ 0 -2 -3]\n",
      " [ 0  0  0]]\n",
      "state 428 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 429 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 430 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 431 : score is : 0 number of moves : 16\n",
      "[[ 1  0 -2]\n",
      " [-2  1 -1]\n",
      " [ 0  0  1]]\n",
      "state 432 : score is : -0.2 number of moves : 12\n",
      "[[ 0 -2 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  0  3]]\n",
      "state 433 : score is : -0.2 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -3]]\n",
      "state 434 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 435 : score is : -0.2 number of moves : 10\n",
      "[[ 0  0  3]\n",
      " [-2 -1  0]\n",
      " [ 0  1 -1]]\n",
      "state 436 : score is : -0.4 number of moves : 16\n",
      "[[ 1 -1  1]\n",
      " [-1 -2  0]\n",
      " [ 0  0 -2]]\n",
      "state 437 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 438 : score is : 0.4 number of moves : 16\n",
      "[[ 1 -2  1]\n",
      " [-2  1  0]\n",
      " [ 0  0  1]]\n",
      "state 439 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 440 : score is : -0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 0 -1  1]\n",
      " [ 0  2 -1]]\n",
      "state 441 : score is : 0.4 number of moves : 22\n",
      "[[ 0 -1  0]\n",
      " [ 2  1  2]\n",
      " [ 0 -1  1]]\n",
      "state 442 : score is : -0.2 number of moves : 28\n",
      "[[ 0 -2 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  1 -1]]\n",
      "state 443 : score is : -0.4 number of moves : 2\n",
      "[[-2 -1  0]\n",
      " [ 0  3  0]\n",
      " [ 0  0 -2]]\n",
      "state 444 : score is : 0.2 number of moves : 26\n",
      "[[-1  1 -1]\n",
      " [ 1  2  1]\n",
      " [ 0  0 -1]]\n",
      "state 445 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 446 : score is : -0.2 number of moves : 4\n",
      "[[-1  0  2]\n",
      " [-3  0  1]\n",
      " [ 0  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(states)):\n",
    "    \n",
    "    print(\"state \"+str(i)+\" : score is : \"+str(get_score_array(get_decoded_state(states[i])))\n",
    "     + \" number of moves : \" + str(len(list(get_actions_indices_array(get_decoded_state(states[i]) , action_dict)))))\n",
    "    print(get_decoded_state(states[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c5a43",
   "metadata": {},
   "source": [
    "### Play against Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1742e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 200,\n",
    "    'action_size': 3 * 3 * 8,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "mcts = MCTS(model, args, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25e7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self Play agent\n",
    "def self_play_agent(state, model, mcts, args, player):\n",
    "    model.eval()\n",
    "    action_probs, root = mcts.search(np.copy(state) * player)\n",
    "    best_action_index = np.argmax(action_probs)\n",
    "    best_action = index_to_action[best_action_index]\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de51a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random Agent\n",
    "import random\n",
    "\n",
    "def random_agent(state, player):\n",
    "    valid_actions = [action for action in index_to_action.values() if is_action_valid_array(state, action)]\n",
    "    return random.choice(valid_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13dad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent1, agent2, agent1_args=None, agent2_args=None, display=True):\n",
    "    if agent1_args is None:\n",
    "        agent1_args = {}\n",
    "    if agent2_args is None:\n",
    "        agent2_args = {}\n",
    "\n",
    "    state = np.array(initial_state)\n",
    "    is_terminal = False\n",
    "    move_number = 1\n",
    "    score = 0\n",
    "\n",
    "    while not is_terminal:\n",
    "        if display:\n",
    "            print(f\"Move number {move_number}:\")\n",
    "            print(\"State:\")\n",
    "            print(state)\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        if move_number % 2 == 1:\n",
    "            action = agent1(state, **agent1_args, player=1)\n",
    "        else:\n",
    "            action = agent2(state, **agent2_args, player=-1)\n",
    "\n",
    "        play_action_array(state, action)\n",
    "        score, is_terminal = get_score_array(state), is_finished_array(state)\n",
    "\n",
    "        if display:\n",
    "            print(f\"Action played: {action}\\n\")\n",
    "\n",
    "        move_number += 1\n",
    "\n",
    "    if display:\n",
    "        print(\"Final state:\")\n",
    "        print(state)\n",
    "        print(f\"Final score: {score}\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4daa7a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game between self_play_agent and random_agent:\n",
      "Move number 1:\n",
      "State:\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "Score: 0\n",
      "Action played: (1, 1, 1, 0)\n",
      "\n",
      "Move number 2:\n",
      "State:\n",
      "[[ 1 -1  1]\n",
      " [ 2  0 -1]\n",
      " [ 0 -1  1]]\n",
      "Score: 0.2\n",
      "Action played: (0, 1, 1, 0)\n",
      "\n",
      "Move number 3:\n",
      "State:\n",
      "[[ 1  0  1]\n",
      " [-3  0 -1]\n",
      " [ 0 -1  1]]\n",
      "Score: -0.2\n",
      "Action played: (2, 2, 2, 1)\n",
      "\n",
      "Move number 4:\n",
      "State:\n",
      "[[ 1  0  1]\n",
      " [-3  0 -1]\n",
      " [ 0  2  0]]\n",
      "Score: 0.2\n",
      "Action played: (1, 2, 2, 1)\n",
      "\n",
      "Final state:\n",
      "[[ 1  0  1]\n",
      " [-3  0  0]\n",
      " [ 0 -3  0]]\n",
      "Final score: -0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate Game between Self Play agent and Random\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "print(\"Game between self_play_agent and random_agent:\")\n",
    "# play_game(self_play_agent, random_agent, agent1_args=agent1_args)\n",
    "play_game(random_agent, self_play_agent, agent2_args=agent_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd6bd1",
   "metadata": {},
   "source": [
    "As we can see : Perfect result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c2cef",
   "metadata": {},
   "source": [
    "### Play against Greedy Player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0595cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_agent(state, player):\n",
    "    actions = get_actions_array(state)\n",
    "    \n",
    "    def predict_score(state, action):\n",
    "        new_state = play_action_array(state.copy(), action)\n",
    "        i2, j2 = action[2], action[3]\n",
    "        return  new_state[i2][j2]\n",
    "    \n",
    "    order = [player*3,player*2,player*1, player*-1,-player*2,-player*3]\n",
    "    srt = {b: i for i, b in enumerate(order)}\n",
    "    sorted_actions = sorted(actions, key=lambda a: srt[predict_score(state, a)])\n",
    "\n",
    "        \n",
    "#     sorted_actions = sorted(actions, key=lambda a: predict_score(state, a), reverse=True)\n",
    "    best_action = sorted_actions[0]\n",
    "\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87af591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([[1,  1,  0],\n",
    "                  [0, -2,  0],\n",
    "                  [0,  3,  0] ])\n",
    "greedy_agent(state, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37ecd80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game between self_play_agent and greedy_agent:\n",
      "Move number 1:\n",
      "State:\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "Score: 0\n",
      "Action played: (0, 0, 0, 1)\n",
      "\n",
      "Move number 2:\n",
      "State:\n",
      "[[ 0  2  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "Score: 0.2\n",
      "Action played: (1, 2, 0, 1)\n",
      "\n",
      "Move number 3:\n",
      "State:\n",
      "[[ 0 -3  1]\n",
      " [-1  1  0]\n",
      " [ 0 -1  1]]\n",
      "Score: -0.2\n",
      "Action played: (0, 2, 1, 1)\n",
      "\n",
      "Move number 4:\n",
      "State:\n",
      "[[ 0 -3  0]\n",
      " [-1  2  0]\n",
      " [ 0 -1  1]]\n",
      "Score: -0.2\n",
      "Action played: (2, 1, 1, 1)\n",
      "\n",
      "Final state:\n",
      "[[ 0 -3  0]\n",
      " [-1 -3  0]\n",
      " [ 0  0  1]]\n",
      "Final score: -0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nGame between self_play_agent and greedy_agent:\")\n",
    "# play_game(self_play_agent, greedy_agent, agent1_args=agent1_args)\n",
    "play_game(greedy_agent, self_play_agent, agent2_args=agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8042b19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "num_games = 20\n",
    "agent1_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, self_play_vs_random, width, label='Self Play vs Random')\n",
    "rects2 = ax.bar(x + width/2, self_play_vs_greedy, width, label='Self Play vs Greedy')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent Performance against Random and Greedy Agents')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947f4c2",
   "metadata": {},
   "source": [
    "We can say that for mini-Avalam of size 3x3, we have already a very good model that can beat random player and Greedy player all the time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c80a1b",
   "metadata": {},
   "source": [
    "### Parallelization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e33d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "\n",
    "    def __init__(self, model, args, device):\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(get_encoded_states(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * action_size, size=policy.shape[0])\n",
    "\n",
    "        for i, spg in enumerate(spGames):\n",
    "\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = np.zeros_like(spg_policy)\n",
    "            for action_index in get_actions_indices_array(states[i], action_dict):\n",
    "                valid_moves[action_index] = 1.0\n",
    "\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = -get_score_array(node.board), is_finished_array(node.board)\n",
    "\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.board for mappingIdx in expandable_spGames])\n",
    "\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(get_encoded_states(states)).to(device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "                for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                    node = spGames[mappingIdx].node\n",
    "                    spg_policy, spg_value = policy[i], value[i].item()\n",
    "\n",
    "                    valid_moves = np.zeros_like(spg_policy)\n",
    "                    for action_index in get_actions_indices_array(node.board, action_dict):\n",
    "                        valid_moves[action_index] = 1.0\n",
    "\n",
    "                    spg_policy *= valid_moves\n",
    "                    spg_policy /= np.sum(spg_policy)\n",
    "                    \n",
    "                    node.expand(spg_policy)\n",
    "                    node.backpropagate(spg_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44e891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel():\n",
    "    def __init__(self, model, optimizer, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(model, args, device)\n",
    "\n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG() for spg in range(self.args[\"num_parallel_games\"])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            \n",
    "\n",
    "            self.mcts.search(states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "\n",
    "                spg = spGames[i]\n",
    "\n",
    "                ## return visit counts\n",
    "                action_probs = np.zeros(action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                \n",
    "                \n",
    "                spg.memory.append((spg.state.copy(), action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args[\"temperature\"])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                \n",
    "                action_index = np.random.choice(action_size, p=temperature_action_probs)\n",
    "                action = index_to_action[action_index]\n",
    "\n",
    "                ## get to the next state using action\n",
    "                spg.state = play_action_array(spg.state, action)\n",
    "                \n",
    "                ## change perspective \n",
    "                spg.state *= -1 \n",
    "\n",
    "                is_terminal = is_finished_array(spg.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        value = get_score_array(spg.state)\n",
    "                        hist_outcome = value / (abs(value) + 1e-8)\n",
    "\n",
    "                        return_memory.append((\n",
    "                            get_encoded_state_(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "#                         combinations = generate_combinations(hist_neutral_state, hist_action_probs, hist_outcome)\n",
    "#                         return_memory.extend(combinations)\n",
    "                \n",
    "#                         return_memory.append(generate_combinations(hist_neutral_state, hist_action_probs, hist_outcome))\n",
    "                    ## remove game after it's finished\n",
    "                    del spGames[i]\n",
    "\n",
    "            player *= -1\n",
    "\n",
    "        return return_memory\n",
    "\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args[\"num_iterations\"] ):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            ## machine plays with itself \n",
    "            for selfPlay_iteration in trange(self.args[\"num_selfPlay_iterations\"]//self.args[\"num_parallel_games\"]):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            ## train based on the memory collected\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args[\"num_epochs\"]):\n",
    "                self.train(memory)\n",
    "                \n",
    "            iteration += self.args[\"start_iteration\"]\n",
    "            torch.save(self.model.state_dict(), f\"model_paral_{iteration}.pt\" )\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_paral_{iteration}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self):\n",
    "        self.state = np.copy(initial_board)\n",
    "        self.memory = []\n",
    "        self.root = None \n",
    "        self.node = None \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "125d9bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023001670837402344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a19860cd90e46f2aeb93395acef0e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029517173767089844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf49abab1194e40ae7ed32326f1babe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 1.36 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_0.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer.load_state_dict(torch.load('optimizer_0.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iterations': 1,\n",
    "    'start_iteration': 1,\n",
    "    'num_parallel_games': 1,\n",
    "    'num_selfPlay_iterations': 1,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "554f372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchIdx in range(0, len(memory), args['batch_size']):\n",
    "sample = memory\n",
    "states, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "states, policy_targets, value_targets = np.array(states), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57baeaf4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0 : score is : 0 number of moves : 34\n",
      "[[ 1 -1  1]\n",
      " [-1  1 -1]\n",
      " [ 0 -1  1]]\n",
      "state 1 : score is : -0.2 number of moves : 20\n",
      "[[-1  1 -1]\n",
      " [ 1  0 -2]\n",
      " [ 0  1 -1]]\n",
      "state 2 : score is : 0 number of moves : 12\n",
      "[[ 1 -1  1]\n",
      " [-1  0  2]\n",
      " [ 0  0 -2]]\n",
      "state 3 : score is : -0.2 number of moves : 8\n",
      "[[ 0  1 -1]\n",
      " [-2  0 -2]\n",
      " [ 0  0  2]]\n",
      "state 4 : score is : -0.2 number of moves : 2\n",
      "[[ 0  0  1]\n",
      " [-3  0  2]\n",
      " [ 0  0 -2]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(states)):\n",
    "    \n",
    "    print(\"state \"+str(i)+\" : score is : \"+str(get_score_array(get_decoded_state(states[i])))\n",
    "     + \" number of moves : \" + str(len(list(get_actions_indices_array(get_decoded_state(states[i]) , action_dict)))))\n",
    "    print(get_decoded_state(states[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11737b8",
   "metadata": {},
   "source": [
    "### check parallelization compared to original "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c864ff5",
   "metadata": {},
   "source": [
    "Train again 100 self game for three iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7576412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029998302459716797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd74853b1e3e4fd7a305afe43b64c298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026997089385986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6424e566b0147ed9ae46dfaee08357d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024007797241210938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f5c9a5e7b6447eaa00806433c03e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03952145576477051,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05b811679af4f8d86151876068b4dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 53.69 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_paral_0.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer.load_state_dict(torch.load('optimizer_paral_0.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iterations': 2,\n",
    "    'start_iteration': 1,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_selfPlay_iterations': 100,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "328f7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_paral_2.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 200,\n",
    "    'action_size': 3 * 3 * 8,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "mcts = MCTS(model, args, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38a4ab54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "num_games = 20\n",
    "agent1_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, self_play_vs_random, width, label='Self Play vs Random')\n",
    "rects2 = ax.bar(x + width/2, self_play_vs_greedy, width, label='Self Play vs Greedy')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent Performance against Random and Greedy Agents')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d68f11",
   "metadata": {},
   "source": [
    "Parallelization is working well ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
