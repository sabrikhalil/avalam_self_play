{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad63a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.0+cu117\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1373976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "from avalam_9x9 import *\n",
    "from Self_Play_avalam_9x9 import *\n",
    "from data_augmentation import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "board_size = 9*9 \n",
    "rows = 9\n",
    "columns = 9 \n",
    "max_height = 5\n",
    "\n",
    "## create dictionary to map actions from indices to action tuple type\n",
    "def create_action_dictionary():\n",
    "    action_dict = {}\n",
    "    index = 0\n",
    "    for row in range(rows):\n",
    "        for col in range(columns):\n",
    "            for drow in range(-1, 2):\n",
    "                for dcol in range(-1, 2):\n",
    "                    if drow == 0 and dcol == 0:\n",
    "                        continue\n",
    "                    new_row = row + drow\n",
    "                    new_col = col + dcol\n",
    "                    if 0 <= new_row < rows and 0 <= new_col < columns:\n",
    "                        action_dict[(row, col, new_row, new_col)] = index\n",
    "                        index += 1\n",
    "    return action_dict\n",
    "\n",
    "action_dict = create_action_dictionary()\n",
    "index_to_action = {index: action for action, index in action_dict.items()}\n",
    "\n",
    "\n",
    "initial_board = [ [ 0,  0,  1, -1,  0,  0,  0,  0,  0],\n",
    "                    [ 0,  1, -1,  1, -1,  0,  0,  0,  0],\n",
    "                    [ 0, -1,  1, -1,  1, -1,  1,  0,  0],\n",
    "                    [ 0,  1, -1,  1, -1,  1, -1,  1, -1],\n",
    "                    [ 1, -1,  1, -1,  0, -1,  1, -1,  1],\n",
    "                    [-1,  1, -1,  1, -1,  1, -1,  1,  0],\n",
    "                    [ 0,  0,  1, -1,  1, -1,  1, -1,  0],\n",
    "                    [ 0,  0,  0,  0, -1,  1, -1,  1,  0],\n",
    "                    [ 0,  0,  0,  0,  0, -1,  1,  0,  0] ]\n",
    "\n",
    "actions_size = len(action_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b86a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_actions(top_actions, action_probs):\n",
    "    \"\"\"\n",
    "    Plot the top action probabilities from the MCTS search.\n",
    "\n",
    "    Args:\n",
    "        top_actions (list): A list of action indices representing the top actions.\n",
    "        action_probs (np.array): A numpy array containing the probabilities of each action.\n",
    "    \"\"\"\n",
    "    top_probabilities = [action_probs[action_index] for action_index in top_actions]\n",
    "\n",
    "    # Get the action coordinates from the action indices\n",
    "    top_action_coordinates = [index_to_action[action_index] for action_index in top_actions]\n",
    "\n",
    "    # Create the bar plot with top actions\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(range(len(top_actions)), top_probabilities)\n",
    "    plt.xticks(range(len(top_actions)), top_action_coordinates, rotation=90)\n",
    "    plt.xlabel('Top Actions (from_x, from_y, to_x, to_y)')\n",
    "    plt.ylabel('Policy Probability')\n",
    "    plt.title('Top Policy Probabilities')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d40286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        if i % 2 == 0:\n",
    "            score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        else:\n",
    "            score = -play_game(agent2, agent1, agent2_args, agent1_args, display=False)\n",
    "\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d66ed",
   "metadata": {},
   "source": [
    "### Load Model and check the action probabilites and value for a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbdc35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027524298056960106\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## state Encoding \n",
    "state =  np.array(  [ [ 0,  0,  3, -1,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  1, -1,  1,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  0,  1,  0,  0,  0, -4,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  4,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  5,  0,  0,  0,  0],\n",
    "                      [ 0,  0, -5,  0,  0,  0,  0,  0,  0],\n",
    "                      [ 0, -5,  0,  0,  0,  0,  5,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  0,  0,  0] ])\n",
    "\n",
    "\n",
    "# state = np.array(initial_state)\n",
    "encoded_state = get_encoded_state_(state)\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "## load Model and generate action probabilities and value state\n",
    "model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_supervised_big_model_final_0.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "#### adjust policy to only valid moves\n",
    "valid_moves = np.zeros_like(policy)\n",
    "for action_index in get_actions_indices_array(state, action_dict):\n",
    "    valid_moves[action_index] = 1.0\n",
    "\n",
    "policy *= valid_moves \n",
    "policy /= np.sum(policy)\n",
    "\n",
    "## Plot action space\n",
    "# Sort actions by their probabilities\n",
    "sorted_actions = np.argsort(policy)[::-1]\n",
    "\n",
    "# Select top 10 actions\n",
    "top_actions = sorted_actions[:10]\n",
    "\n",
    "# Plot the top 10 actions\n",
    "plot_top_actions(top_actions, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf7994",
   "metadata": {},
   "source": [
    "### 1.MCTS and Self Play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07397d",
   "metadata": {},
   "source": [
    "#### 1.1 MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e168b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS() : \n",
    "\n",
    "    def __init__(self, model, args, device) :\n",
    "        self.args = args\n",
    "        self.model = model\n",
    " \n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "#         define root \n",
    "        root = Node(self.args, state, visit_count=1)  ## board and state mean same thing \n",
    "        \n",
    "        ## add noise \n",
    "        policy, _ = self.model(\n",
    "                    torch.tensor(get_encoded_state_(state)).unsqueeze(0).to(device)\n",
    "                )\n",
    "\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * actions_size)\n",
    "                \n",
    "        valid_moves = np.zeros_like(policy)\n",
    "        for action_index in get_actions_indices_array(state, action_dict):\n",
    "            valid_moves[action_index] = 1.0\n",
    "        \n",
    "        policy *= valid_moves \n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            ## Selection \n",
    "            node = root\n",
    "\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            value, is_terminal = -get_score_array(node.board), is_finished_array(node.board)\n",
    "\n",
    "            if not is_terminal: \n",
    "         \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(get_encoded_state_(node.board)).unsqueeze(0).to(device)\n",
    "                ) \n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = np.zeros_like(policy)\n",
    "                for action_index in get_actions_indices_array(node.board,action_dict):\n",
    "                    valid_moves[action_index] = 1.0\n",
    "\n",
    "                policy *= valid_moves \n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                value = value.item()\n",
    "\n",
    "                ## Expansion\n",
    "                node = node.expand(policy)\n",
    "                \n",
    "            ## Backpropagation\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        ## return visit counts \n",
    "        action_probs = [0] * actions_size\n",
    "\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        total_visit_count = sum(action_probs)\n",
    "        action_probs = np.array([prob / total_visit_count for prob in action_probs])\n",
    "        return action_probs, root\n",
    "\n",
    "class Node: \n",
    "    def __init__(self, args, board, parent=None, action_taken=None ,prior=0, visit_count=0):\n",
    "        self.args = args \n",
    "        self.board = board \n",
    "        self.parent = parent \n",
    "        self.action_taken = action_taken \n",
    "        self.prior = prior  \n",
    "\n",
    "        self.children = []\n",
    "        self.expandable_moves = list(get_actions_indices_array(board, action_dict))\n",
    "\n",
    "        self.visit_count = 0 \n",
    "        self.value_sum = 0 \n",
    "\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return  len(self.children)>0 \n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None \n",
    "        best_ucb = -np.inf \n",
    "\n",
    "        for child in self.children : \n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb :\n",
    "                best_child = child \n",
    "                best_ucb = ucb \n",
    "\n",
    "        return best_child \n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        prior_score = child.prior * math.sqrt(self.visit_count) / (child.visit_count + 1)\n",
    "        if child.visit_count > 0:\n",
    "            value_score = child.value_sum / child.visit_count\n",
    "        else:\n",
    "            value_score = 0\n",
    "        return value_score + prior_score\n",
    "\n",
    "    def expand(self, policy):\n",
    "\n",
    "        for action_ind, prob in enumerate(policy):\n",
    "            if prob > 0 : \n",
    "                \n",
    "                ## Store position of cells and their current height that will be impacted by the move , so we can undo the move \n",
    "                store_move = []\n",
    "                action = index_to_action[action_ind]\n",
    "                store_move.append((action[0] , action[1] , self.board[action[0]][action[1]])) \n",
    "                store_move.append((action[2] , action[3] , self.board[action[2]][action[3]]))\n",
    "\n",
    "                ## play move \n",
    "                play_action_array(self.board,action)  \n",
    "\n",
    "                ## change perspective \n",
    "                self.board = -1 * np.array(self.board)\n",
    "                child_board = np.copy(self.board)\n",
    "                child = Node(self.args, child_board, self, action_ind, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "                ## Undo the move is restoring the cells that were changed by their old state \n",
    "                self.board = -1 * self.board\n",
    "                self.board[store_move[0][0]][store_move[0][1]] = store_move[0][2]\n",
    "                self.board[store_move[1][0]][store_move[1][1]] = store_move[1][2]\n",
    "                \n",
    "        return child\n",
    "\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = -value \n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28153b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best action index: 65\n",
      "Best action: (1, 3, 1, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## state Encoding \n",
    "state =  np.array(  [ [ 0,  0,  3, -1,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  1, -1,  1,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  0,  1,  0,  0,  0, -4,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  4,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  5,  0,  0,  0,  0],\n",
    "                      [ 0,  0, -5,  0,  0,  0,  0,  0,  0],\n",
    "                      [ 0, -5,  0,  0,  0,  0,  5,  0,  0],\n",
    "                      [ 0,  0,  0,  0,  0,  0,  0,  0,  0] ])\n",
    "\n",
    "# state = np.array(initial_state)\n",
    "model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_supervised_big_model_final_0.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 3000,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth' : 100\n",
    "}\n",
    "\n",
    "mcts = MCTS(model, args, device)\n",
    "\n",
    "action_probs, root = mcts.search(state)\n",
    "\n",
    "best_action_index = np.argmax(action_probs)\n",
    "print(\"Best action index:\", best_action_index)\n",
    "best_action = index_to_action[best_action_index]\n",
    "print(\"Best action:\", best_action)\n",
    "\n",
    "# Sort actions by their probabilities\n",
    "sorted_actions = np.argsort(action_probs)[::-1]\n",
    "\n",
    "# Select top 10 actions\n",
    "top_actions = sorted_actions[:10]\n",
    "\n",
    "# Plot the top 10 actions\n",
    "plot_top_actions(top_actions, action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279d43d",
   "metadata": {},
   "source": [
    "#### 1.2 Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0df8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = [ [ 0,  0,  1, -1,  0,  0,  0,  0,  0],\n",
    "                  [ 0,  1, -1,  1, -1,  0,  0,  0,  0],\n",
    "                  [ 0, -1,  1, -1,  1, -1,  1,  0,  0],\n",
    "                  [ 0,  1, -1,  1, -1,  1, -1,  1, -1],\n",
    "                  [ 1, -1,  1, -1,  0, -1,  1, -1,  1],\n",
    "                  [-1,  1, -1,  1, -1,  1, -1,  1,  0],\n",
    "                  [ 0,  0,  1, -1,  1, -1,  1, -1,  0],\n",
    "                  [ 0,  0,  0,  0, -1,  1, -1,  1,  0],\n",
    "                  [ 0,  0,  0,  0,  0, -1,  1,  0,  0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200cbac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero():\n",
    "    def __init__(self, model, optimizer, args):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer \n",
    "        self.args = args \n",
    "        self.mcts = MCTS(model, args, device)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1 \n",
    "        state = np.copy(initial_state) ## initialize game state \n",
    "\n",
    "        while True :       \n",
    "            action_probs, root = self.mcts.search(np.copy(state))\n",
    "            \n",
    "            memory.append((get_encoded_state_(state), action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = np.array(action_probs) ** (1/self.args[\"temperature\"])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action_index = np.random.choice(actions_size, p=temperature_action_probs)  ## this action is scalar \n",
    "            action = index_to_action[action_index]\n",
    "            \n",
    "            ## get to next state using action \n",
    "            play_action_array(state,action) \n",
    "\n",
    "            value, is_terminal = get_score_array(state), is_finished_array(state)\n",
    "\n",
    "            if is_terminal : \n",
    "                returnMemory = []\n",
    "                for hist_neutral_state_encoded, hist_action_probs, hist_player in memory : \n",
    "                    hist_outcome = value * hist_player \n",
    "                    returnMemory.append((\n",
    "                        hist_neutral_state_encoded, \n",
    "                        hist_action_probs, \n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            ## change perspective \n",
    "            state = -1 * np.array(state)\n",
    "            player *= -1\n",
    "            \n",
    "            \n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        for iteration in range(self.args[\"num_iterations\"]):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            ## machine plays with itself \n",
    "            for selfPlay_iteration in trange(self.args[\"num_selfPlay_iterations\"]):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            ## train based on the memory collected\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args[\"num_epochs\"]):\n",
    "                self.train(memory)\n",
    "                \n",
    "            iteration += self.args[\"start_iteration\"]\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\" )\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n",
    "            return memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50938e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model.load_state_dict(torch.load('model_1.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# optimizer.load_state_dict(torch.load('optimizer_1.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 300,\n",
    "    'num_iterations': 3,\n",
    "    'start_iteration': 0,\n",
    "    'num_parallel_games': 1,\n",
    "    'num_selfPlay_iterations': 1,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory_ = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2c04f",
   "metadata": {},
   "source": [
    "This part is to check the memory and if all is good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchIdx in range(0, len(memory), args['batch_size']):\n",
    "sample = memory_\n",
    "states, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "states, policy_targets, value_targets = np.array(states), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(states)):\n",
    "    \n",
    "    print(\"state \"+str(i)+\" : score is : \"+str(get_score_array(get_decoded_state(states[i])))\n",
    "     + \" number of moves : \" + str(len(list(get_actions_indices_array(get_decoded_state(states[i]) , action_dict)))))\n",
    "    print(get_decoded_state(states[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86493340",
   "metadata": {},
   "source": [
    "#### 1.3Play against Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec9f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model.load_state_dict(torch.load('model_paral_2.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':100\n",
    "}\n",
    "mcts = MCTS(model, args, device)\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f130de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self Play agent\n",
    "def self_play_agent(state, model, mcts, args, player):\n",
    "    if model : \n",
    "        model.eval()\n",
    "    action_probs, root = mcts.search(np.copy(state) * player)\n",
    "    best_action_index = np.argmax(action_probs)\n",
    "    best_action = index_to_action[best_action_index]\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9be799fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random Agent\n",
    "import random\n",
    "\n",
    "def random_agent(state, player):\n",
    "    valid_actions = [action for action in index_to_action.values() if is_action_valid_array(state, action)]\n",
    "    return random.choice(valid_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134365e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent1, agent2, agent1_args=None, agent2_args=None, display=True):\n",
    "    if agent1_args is None:\n",
    "        agent1_args = {}\n",
    "    if agent2_args is None:\n",
    "        agent2_args = {}\n",
    "\n",
    "    state = np.array(initial_state)\n",
    "    is_terminal = False\n",
    "    move_number = 1\n",
    "    score = 0\n",
    "\n",
    "    while not is_terminal:\n",
    "        if display:\n",
    "            print(f\"Move number {move_number}:\")\n",
    "            print(\"State:\")\n",
    "            print(state)\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        if move_number % 2 == 1:\n",
    "            action = agent1(state, **agent1_args, player=1)\n",
    "        else:\n",
    "            action = agent2(state, **agent2_args, player=-1)\n",
    "\n",
    "        play_action_array(state, action)\n",
    "        score, is_terminal = get_score_array(state), is_finished_array(state)\n",
    "\n",
    "        if display:\n",
    "            print(f\"Action played: {action}\\n\")\n",
    "\n",
    "        move_number += 1\n",
    "\n",
    "    if display:\n",
    "        print(\"Final state:\")\n",
    "        print(state)\n",
    "        print(f\"Final score: {score}\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90397754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Game between Self Play agent and Random\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "print(\"Game between self_play_agent and random_agent:\")\n",
    "play_game(self_play_agent, random_agent, agent1_args=agent_args)\n",
    "# play_game(random_agent, self_play_agent, agent2_args=agent_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4f400",
   "metadata": {},
   "source": [
    "As we can see : Perfect result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443e9ac",
   "metadata": {},
   "source": [
    "#### 1.4 Play against Greedy Player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bdb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_agent(state, player):\n",
    "    actions = get_actions_array(state)\n",
    "    \n",
    "    def predict_score(state, action):\n",
    "        new_state = play_action_array(state.copy(), action)\n",
    "        i2, j2 = action[2], action[3]\n",
    "        return  new_state[i2][j2]\n",
    "    \n",
    "    order = [player*5, player*4, player*3,player*2,player*1, player*-1,-player*2,-player*3, -player*4, -player*5]\n",
    "    srt = {b: i for i, b in enumerate(order)}\n",
    "    sorted_actions = sorted(actions, key=lambda a: srt[predict_score(state, a)])\n",
    "\n",
    "    best_action = sorted_actions[0]\n",
    "\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfb2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nGame between self_play_agent and greedy_agent:\")\n",
    "play_game(self_play_agent, greedy_agent, agent1_args=agent_args)\n",
    "# play_game(greedy_agent, self_play_agent, agent2_args=agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "num_games = 20\n",
    "agent1_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent1_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent1_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, self_play_vs_random, width, label='Self Play vs Random')\n",
    "rects2 = ax.bar(x + width/2, self_play_vs_greedy, width, label='Self Play vs Greedy')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent Performance against Random and Greedy Agents')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1c332",
   "metadata": {},
   "source": [
    "We can say that for mini-Avalam of size 3x3, we have already a very good model that can beat random player and Greedy player all the time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6cec",
   "metadata": {},
   "source": [
    "### 2.Parallelization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed05ce",
   "metadata": {},
   "source": [
    "#### 2.1 MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab7d73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "\n",
    "    def __init__(self, model, args, device):\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(get_encoded_states(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * actions_size, size=policy.shape[0])\n",
    "\n",
    "        for i, spg in enumerate(spGames):\n",
    "\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = np.zeros_like(spg_policy)\n",
    "            for action_index in get_actions_indices_array(states[i], action_dict):\n",
    "                valid_moves[action_index] = 1.0\n",
    "\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = -get_score_array(node.board), is_finished_array(node.board)\n",
    "\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.board for mappingIdx in expandable_spGames])\n",
    "\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(get_encoded_states(states)).to(device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "                for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                    node = spGames[mappingIdx].node\n",
    "                    spg_policy, spg_value = policy[i], value[i].item()\n",
    "\n",
    "                    valid_moves = np.zeros_like(spg_policy)\n",
    "                    for action_index in get_actions_indices_array(node.board, action_dict):\n",
    "                        valid_moves[action_index] = 1.0\n",
    "\n",
    "                    spg_policy *= valid_moves\n",
    "                    spg_policy /= np.sum(spg_policy)\n",
    "                    \n",
    "                    node.expand(spg_policy)\n",
    "                    node.backpropagate(spg_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a94986",
   "metadata": {},
   "source": [
    "#### 2.2 Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fee2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel():\n",
    "    def __init__(self, model, optimizer, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(model, args, device)\n",
    "\n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG() for spg in range(self.args[\"num_parallel_games\"])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            \n",
    "\n",
    "            self.mcts.search(states, spGames)\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "\n",
    "                spg = spGames[i]\n",
    "\n",
    "                ## return visit counts\n",
    "                action_probs = np.zeros(actions_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                \n",
    "                \n",
    "                spg.memory.append((spg.state.copy(), action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args[\"temperature\"])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                \n",
    "                action_index = np.random.choice(actions_size, p=temperature_action_probs)\n",
    "                action = index_to_action[action_index]\n",
    "\n",
    "                ## get to the next state using action\n",
    "                spg.state = play_action_array(spg.state, action)\n",
    "                \n",
    "                ## change perspective \n",
    "                spg.state *= -1 \n",
    "\n",
    "                is_terminal = is_finished_array(spg.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        value = -get_score_array(spg.state) * hist_player \n",
    "                        hist_outcome = value\n",
    "                        \n",
    "                        ## Normal Training\n",
    "#                         return_memory.append((\n",
    "#                             get_encoded_state_(hist_neutral_state),\n",
    "#                             hist_action_probs,\n",
    "#                             hist_outcome\n",
    "#                         ))\n",
    "                        ### data augmentation \n",
    "                        combinations = generate_combinations(hist_neutral_state, hist_action_probs, hist_outcome)\n",
    "                        return_memory.extend(combinations)\n",
    "                \n",
    "#                         return_memory.append(generate_combinations(hist_neutral_state, hist_action_probs, hist_outcome))\n",
    "                    ## remove game after it's finished\n",
    "                    del spGames[i]\n",
    "        \n",
    "                \n",
    "\n",
    "            player *= -1\n",
    "\n",
    "        return return_memory\n",
    "\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args[\"num_iterations\"] ):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            ## machine plays with itself \n",
    "            for selfPlay_iteration in trange(self.args[\"num_selfPlay_iterations\"]//self.args[\"num_parallel_games\"]):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            ## train based on the memory collected\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args[\"num_epochs\"]):\n",
    "                self.train(memory)\n",
    "                \n",
    "            iteration += self.args[\"start_iteration\"]\n",
    "            torch.save(self.model.state_dict(), f\"model_paral_{iteration}.pt\" )\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_paral_{iteration}.pt\")\n",
    "        return memory\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self):\n",
    "        self.state = np.copy(initial_state)\n",
    "        self.memory = []\n",
    "        self.root = None \n",
    "        self.node = None \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96670699",
   "metadata": {},
   "source": [
    "#### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ab00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model.load_state_dict(torch.load('model_paral_2.pt', map_location=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# optimizer.load_state_dict(torch.load('optimizer_paral_2.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 500,\n",
    "    'num_iterations': 1,\n",
    "    'start_iteration': 3, ### DONT FORGET \n",
    "    'num_parallel_games': 50,\n",
    "    'num_selfPlay_iterations': 50,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':100\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchIdx in range(0, len(memory), args['batch_size']):\n",
    "sample = memory\n",
    "states, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "states, policy_targets, value_targets = np.array(states), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885eadf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(states)):\n",
    "    \n",
    "    print(\"state \"+str(i)+\" : score is : \"+str(get_score_array(get_decoded_state(states[i])))\n",
    "     + \" number of moves : \" + str(len(list(get_actions_indices_array(get_decoded_state(states[i]) , action_dict)))))\n",
    "    print(get_decoded_state(states[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35480785",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in value_targets :\n",
    "    print(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798f9a0",
   "metadata": {},
   "source": [
    "#### 2.4 Play against older version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model_1.load_state_dict(torch.load('model_paral_2.pt', map_location=device))\n",
    "\n",
    "model_2 = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model_2.load_state_dict(torch.load('model_paral_1.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 1500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth': 100\n",
    "}\n",
    "mcts_1 = MCTS(model_1, args, device)\n",
    "mcts_2 = MCTS(model_2, args, device)\n",
    "\n",
    "agent1_args = {'model': model_1, 'mcts': mcts_1, 'args': args}\n",
    "agent2_args = {'model': model_2, 'mcts': mcts_2, 'args': args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e942cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGame between self_play_agent_v1 and self_play_agent_v2\")\n",
    "# play_game(self_play_agent, greedy_agent, agent1_args=agent1_args)\n",
    "# play_game(greedy_agent, self_play_agent, agent2_args=agent1_args)\n",
    "play_game(self_play_agent, self_play_agent, agent1_args=agent1_args, agent2_args=agent2_args)\n",
    "play_game(self_play_agent, self_play_agent, agent1_args=agent2_args, agent2_args=agent1_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86abdf8b",
   "metadata": {},
   "source": [
    "#### 2.5 PLay against Greedy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_paral_3.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 2000,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':5\n",
    "}\n",
    "agent_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "mcts = MCTS(model, args, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96414d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGame between self_play_agent and greedy_agent:\")\n",
    "# play_game(self_play_agent, greedy_agent, agent1_args=agent_args)\n",
    "play_game(greedy_agent, self_play_agent, agent2_args=agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "num_games = 5\n",
    "# agent1_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, self_play_vs_random, width, label='Self Play vs Random')\n",
    "rects2 = ax.bar(x + width/2, self_play_vs_greedy, width, label='Self Play vs Greedy')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent Performance against Random and Greedy Agents')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a110af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_play_vs_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0a832",
   "metadata": {},
   "source": [
    "### 3.Training using Pre-built agents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b035e",
   "metadata": {},
   "source": [
    "#### 3.1 Heuristics Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28b4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_ranked (board , limit_moves ,max_player): \n",
    "    \"\"\"Yield all valid actions on this board.\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    actions = []\n",
    "\n",
    "    towers = get_towers_array(board)\n",
    "    towers_position = {(x,y) for x,y,z in towers}\n",
    "    \n",
    "    \n",
    "    for i, j, h in get_towers_array(board):\n",
    "        for action in get_tower_actions_array(board, i, j):\n",
    "            i1, j1, i2, j2 = action\n",
    "            h1 = abs(board[i1][j1])\n",
    "            h2 = abs(board[i2][j2])\n",
    "\n",
    "            score = 0 \n",
    "            visited = {}\n",
    "            \n",
    "            ###### Heuristic 1 for Isolate Pawns :\n",
    "            ## Store position of cells and their current height that will be impacted by the move , so we can undo the move \n",
    "            store_move = []\n",
    "            store_move.append(( i1, j1 , board[i1][j1])) \n",
    "            store_move.append(( i2, j2 , board[i2][j2]))\n",
    "\n",
    "            # play action on board\n",
    "            play_action_array(board, action) \n",
    "\n",
    "            # check if move yielded isolated pawns (neighbors of (i,j)cell)\n",
    "            for i_ in range(i1-1 , i1+2, 1) :\n",
    "                for j_ in range(j1-1, j1+2 ,1) :\n",
    "                    if (i_,j_) in towers_position : \n",
    "                        if not is_tower_movable_array(board,i_, j_) and board[i_][j_] != 0 and abs(board[i_][j_]) != 5 :\n",
    "                            visited[(i,j)] = True\n",
    "                            score += 3 * board[i_][j_] / abs(board[i_][j_] )    \n",
    "\n",
    "            \n",
    "            ## Undo the move is restoring the cells that were changed by their old state \n",
    "            board[store_move[0][0]][store_move[0][1]] = store_move[0][2]\n",
    "            board[store_move[1][0]][store_move[1][1]] = store_move[1][2]\n",
    "            ########\n",
    "\n",
    "\n",
    "            ###### Heuristic 2 : If Move result on tours with heigher weight \n",
    "            if board[i1][j1] < 0 and (i,j) not in visited: ## yellow on top\n",
    "                score += -(h1 + h2)\n",
    "            elif board[i1][j1] > 0 and (i,j) not in visited :\n",
    "                score += h1 + h2\n",
    "\n",
    "            ###### Heuristic 3: Moves where you put your own stone on top of another yours is generally weak\n",
    "            if  board[i1][j1] * board[i2][j2] >= 1 :\n",
    "                score -= 2.5 * board[i1][j1] / abs(board[i1][j1] )\n",
    "\n",
    "            # store new score resulted from the action \n",
    "            scores.append(score)\n",
    "            actions.append(action) \n",
    "            \n",
    "    # Normalize scores to probabilities\n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    if max_score == min_score:  # to avoid division by zero\n",
    "        probs = [1 / len(scores)] * len(scores)\n",
    "    else:\n",
    "        if max_player == -1:\n",
    "            scores = [-score for score in scores]  # flip scores for max_player == -1\n",
    "        probs = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "\n",
    "    return sorted(zip(actions, probs), key=lambda x: x[1], reverse=True)[:limit_moves]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "087e579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_board(board  ):\n",
    "        \n",
    "    score = 0\n",
    "    visited = set()\n",
    "\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            \n",
    "            if board[i][j] == 0 :\n",
    "                pass\n",
    "            \n",
    "            ## assign 1.55 points to Tour \n",
    "            elif board[i][j] in (-max_height,max_height) :\n",
    "                score += 1.55 * board[i][j] / abs(board[i][j] )\n",
    "\n",
    "            ## assign 3 points to isolated Powns \n",
    "            elif not is_tower_movable_array(board, i, j):\n",
    "                score += 1.5 * board[i][j] / abs(board[i][j] )\n",
    "                \n",
    "            ## assign 1 point to the rest of the Pawns if not isolated group of pawns , otherwise assign the correct value as explained in the report \n",
    "            else :\n",
    "                ## if already visited assign one point to that pawn \n",
    "                if (i,j) in visited :\n",
    "                    score += board[i][j] / abs(board[i][j] )\n",
    "                \n",
    "                ## if never visited , check the existence of isolated group of pawns \n",
    "                else :\n",
    "                    color = board[i][j] / abs(board[i][j] )\n",
    "                    cycle = [(i,j)]\n",
    "                    cycle_sum = max_height - abs(board[i][j])\n",
    "                    same_color = True \n",
    "                    nodes_visited = -1 \n",
    "\n",
    "                    \n",
    "                    while ( cycle_sum >= 0 and cycle != [] and same_color ) :\n",
    "\n",
    "                        current = cycle.pop()\n",
    "\n",
    "                         ## look neighbors \n",
    "                        for action in get_tower_actions_array(board, current[0] , current[1]):               \n",
    "                            i1, j1, i2, j2 = action\n",
    "                            ## if same color , continue \n",
    "                            if board[i2][j2] / abs(board[i2][j2] ) != color :\n",
    "                                same_color = False \n",
    "                                \n",
    "                            if current not in visited : \n",
    "                                cycle.append((i2,j2))\n",
    "                                visited.add((i2,j2))\n",
    "                                cycle_sum -= abs(board[i2][j2])\n",
    "                        \n",
    "                        nodes_visited += 1 \n",
    "\n",
    "                    ## if already more than 5 or color is changed , assign one point . \n",
    "                    if cycle != [] or same_color == False : \n",
    "                        score += board[i][j] / abs(board[i][j] ) \n",
    "\n",
    "                    ## if its one group isolated of pawns , assign 1.55 if their sum is 5 (equivalent to 1 Tour ) \n",
    "                    ## and 1.5 if their sum is less than 5 (Equivalent to isolated pawn)   \n",
    "                    else : \n",
    "                        if cycle_sum == 3 : \n",
    "                            score += 1.55 * color - nodes_visited * 1 * color \n",
    "                        else :\n",
    "                            score += 1.5 * color - nodes_visited * 1 * color \n",
    "                    \n",
    "                \n",
    "    return score \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c43dc0",
   "metadata": {},
   "source": [
    "#### 3.2 MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51507223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Supervised() : \n",
    "\n",
    "    def __init__(self, args, device) :\n",
    "        self.args = args\n",
    " \n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "#         define root \n",
    "        root = Node(self.args, state, visit_count=1)  ## board and state mean same thing \n",
    "        \n",
    "        value = score_board(root.board)\n",
    "\n",
    "\n",
    "        action_probs = get_actions_ranked(root.board, 20, 1) ## get list of actions ranked with probabilities\n",
    "\n",
    "        policy = np.zeros(actions_size)\n",
    "        for action, prob in action_probs:\n",
    "                action_index = action_dict[action]\n",
    "                policy[action_index] = prob\n",
    "        \n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(self.args[\"num_searches\"]):\n",
    "            ## Selection \n",
    "            node = root\n",
    "\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            value, is_terminal = -get_score_array(node.board), is_finished_array(node.board)\n",
    "\n",
    "            if not is_terminal: \n",
    "         \n",
    "                value = score_board(node.board)\n",
    "\n",
    "                policy = np.zeros(9*9*8)\n",
    "\n",
    "                action_probs = get_actions_ranked(node.board, 20, 1) ## get list of actions ranked with probabilities\n",
    "\n",
    "                policy = np.zeros(9*9*8)\n",
    "                for action, prob in action_probs:\n",
    "                    action_index = action_dict[action]\n",
    "                    policy[action_index] = prob\n",
    "\n",
    "                ## Expansion\n",
    "                node = node.expand(policy)\n",
    "                \n",
    "            ## Backpropagation\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        ## return visit counts \n",
    "        action_probs = [0] * actions_size\n",
    "\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        total_visit_count = sum(action_probs)\n",
    "        action_probs = np.array([prob / total_visit_count for prob in action_probs])\n",
    "        return action_probs, root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa952c",
   "metadata": {},
   "source": [
    "Test a Game using MCTS with Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222ff2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game between self_play_agent and random_agent:\n",
      "Move number 1:\n",
      "State:\n",
      "[[ 0  0  1 -1  0  0  0  0  0]\n",
      " [ 0  1 -1  1 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0\n",
      "Action played: (0, 2, 0, 3)\n",
      "\n",
      "Move number 2:\n",
      "State:\n",
      "[[ 0  0  0  2  0  0  0  0  0]\n",
      " [ 0  1 -1  1 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (1, 2, 0, 3)\n",
      "\n",
      "Move number 3:\n",
      "State:\n",
      "[[ 0  0  0 -3  0  0  0  0  0]\n",
      " [ 0  1  0  1 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0\n",
      "Action played: (1, 3, 0, 3)\n",
      "\n",
      "Move number 4:\n",
      "State:\n",
      "[[ 0  0  0  4  0  0  0  0  0]\n",
      " [ 0  1  0  0 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (1, 4, 0, 3)\n",
      "\n",
      "Move number 5:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (2, 4, 2, 3)\n",
      "\n",
      "Move number 6:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1  2  0 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (3, 2, 2, 3)\n",
      "\n",
      "Move number 7:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -3  0 -1  1  0  0]\n",
      " [ 0  1  0  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (3, 3, 2, 3)\n",
      "\n",
      "Move number 8:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1  4  0 -1  1  0  0]\n",
      " [ 0  1  0  0 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (3, 4, 2, 3)\n",
      "\n",
      "Move number 9:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -5  0 -1  1  0  0]\n",
      " [ 0  1  0  0  0  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (2, 6, 2, 5)\n",
      "\n",
      "Move number 10:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -5  0  2  0  0  0]\n",
      " [ 0  1  0  0  0  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (3, 6, 2, 5)\n",
      "\n",
      "Move number 11:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -5  0 -3  0  0  0]\n",
      " [ 0  1  0  0  0  1  0  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (3, 5, 2, 5)\n",
      "\n",
      "Move number 12:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (2, 1, 1, 1)\n",
      "\n",
      "Move number 13:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0 -2  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (2, 2, 1, 1)\n",
      "\n",
      "Move number 14:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (3, 8, 3, 7)\n",
      "\n",
      "Move number 15:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0 -2  0]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (4, 6, 3, 7)\n",
      "\n",
      "Move number 16:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  3  0]\n",
      " [ 1 -1  1 -1  0 -1  0 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (4, 7, 3, 7)\n",
      "\n",
      "Move number 17:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0 -4  0]\n",
      " [ 1 -1  1 -1  0 -1  0  0  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (4, 8, 3, 7)\n",
      "\n",
      "Move number 18:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  5  0]\n",
      " [ 1 -1  1 -1  0 -1  0  0  0]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (4, 1, 3, 1)\n",
      "\n",
      "Move number 19:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0 -2  0  0  0  0  0  5  0]\n",
      " [ 1  0  1 -1  0 -1  0  0  0]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (4, 2, 3, 1)\n",
      "\n",
      "Move number 20:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 1  0  0 -1  0 -1  0  0  0]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (4, 3, 5, 2)\n",
      "\n",
      "Move number 21:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 1  0  0  0  0 -1  0  0  0]\n",
      " [-1  1 -2  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.2\n",
      "Action played: (4, 0, 5, 0)\n",
      "\n",
      "Move number 22:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0 -1  0  0  0]\n",
      " [ 2  1 -2  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.3\n",
      "Action played: (5, 2, 5, 1)\n",
      "\n",
      "Move number 23:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0 -1  0  0  0]\n",
      " [ 2 -3  0  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.2\n",
      "Action played: (6, 2, 5, 1)\n",
      "\n",
      "Move number 24:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0 -1  0  0  0]\n",
      " [ 2  4  0  1 -1  1 -1  1  0]\n",
      " [ 0  0  0 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.3\n",
      "Action played: (4, 5, 5, 4)\n",
      "\n",
      "Move number 25:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1 -2  1 -1  1  0]\n",
      " [ 0  0  0 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action played: (5, 5, 5, 4)\n",
      "\n",
      "Move number 26:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  3  0 -1  1  0]\n",
      " [ 0  0  0 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.5\n",
      "Action played: (6, 3, 5, 4)\n",
      "\n",
      "Move number 27:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1 -4  0 -1  1  0]\n",
      " [ 0  0  0  0  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.4\n",
      "Action played: (6, 4, 5, 4)\n",
      "\n",
      "Move number 28:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.5\n",
      "Action played: (5, 6, 5, 7)\n",
      "\n",
      "Move number 29:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0 -2  0]\n",
      " [ 0  0  0  0  0 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.4\n",
      "Action played: (6, 5, 7, 4)\n",
      "\n",
      "Move number 30:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0 -2  0]\n",
      " [ 0  0  0  0  0  0  1 -1  0]\n",
      " [ 0  0  0  0 -2  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.5\n",
      "Action played: (5, 7, 6, 6)\n",
      "\n",
      "Move number 31:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -3 -1  0]\n",
      " [ 0  0  0  0 -2  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.4\n",
      "Action played: (6, 6, 6, 7)\n",
      "\n",
      "Move number 32:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 -4  0]\n",
      " [ 0  0  0  0 -2  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.5\n",
      "Action played: (6, 7, 7, 6)\n",
      "\n",
      "Move number 33:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -2  1 -5  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.6\n",
      "Action played: (8, 6, 8, 5)\n",
      "\n",
      "Move number 34:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -2  1 -5  1  0]\n",
      " [ 0  0  0  0  0  2  0  0  0]]\n",
      "Score: 0.7\n",
      "Action played: (7, 4, 8, 5)\n",
      "\n",
      "Move number 35:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 -5  1  0]\n",
      " [ 0  0  0  0  0 -4  0  0  0]]\n",
      "Score: 0.6\n",
      "Action played: (7, 5, 8, 5)\n",
      "\n",
      "Final state:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -5  0  4  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  0  1  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -5  1  0]\n",
      " [ 0  0  0  0  0  5  0  0  0]]\n",
      "Final score: 0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':100\n",
    "}\n",
    "mcts_supervised = MCTS_Supervised(args, device)\n",
    "agent_with_heuristics_args = {'model': None, 'mcts': mcts_supervised, 'args': args}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Game between self_play_agent and random_agent:\")\n",
    "play_game(self_play_agent, greedy_agent, agent1_args=agent_with_heuristics_args)\n",
    "# play_game(greedy_agent, self_play_agent, agent2_args=agent_with_heuristics_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3cf01",
   "metadata": {},
   "source": [
    "Here, we can see that the agent using Heuristics is able to beat greedy but a very big margin !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a2bc0",
   "metadata": {},
   "source": [
    "#### 3.3 Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa265e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class AlphaZero_Supervised():\n",
    "    def __init__(self, model, optimizer, args):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer \n",
    "        self.args = args \n",
    "        self.mcts = MCTS_Supervised( args, device)\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        ### make sure that we produce pure randomness in our move generation \n",
    "        random.seed(time.time())\n",
    "        np.random.seed(int(time.time()))\n",
    "        \n",
    "        memory = []\n",
    "        player = 1 \n",
    "        state = np.copy(initial_state) ## initialize game state \n",
    "\n",
    "        while True :       \n",
    "            action_probs, root = self.mcts.search(np.copy(state))\n",
    "            \n",
    "            memory.append((state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = np.array(action_probs) ** (1/self.args[\"temperature\"])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action_index = np.random.choice(actions_size, p=temperature_action_probs)  ## this action is scalar \n",
    "            action = index_to_action[action_index]\n",
    "            \n",
    "            ## get to next state using action \n",
    "            play_action_array(state,action) \n",
    "\n",
    "            value, is_terminal = get_score_array(state), is_finished_array(state)\n",
    "\n",
    "            if is_terminal : \n",
    "                returnMemory = []\n",
    "#                 for hist_neutral_state_encoded, hist_action_probs, hist_player in memory : \n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory : \n",
    "                    hist_outcome = value * hist_player \n",
    "                    returnMemory.append((\n",
    "                        get_encoded_state_(hist_neutral_state), \n",
    "                        hist_action_probs, \n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                    ### data augmentation \n",
    "                    combinations = generate_combinations(hist_neutral_state, hist_action_probs, hist_outcome)\n",
    "                    returnMemory.extend(combinations)\n",
    "                return returnMemory\n",
    "            \n",
    "            ## change perspective \n",
    "            state = -1 * np.array(state)\n",
    "            player *= -1\n",
    "            \n",
    "            \n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        for iteration in range(self.args[\"num_iterations\"]):\n",
    "            memory = []\n",
    "            iteration += self.args[\"start_iteration\"]\n",
    "\n",
    "            self.model.eval()\n",
    "            ## machine plays with itself \n",
    "            for selfPlay_iteration in trange(self.args[\"num_selfPlay_iterations\"]):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            # Save memory to file\n",
    "            with open(f'data_{iteration}.pkl', 'wb') as f:\n",
    "                pickle.dump(memory, f)\n",
    "\n",
    "            ## train based on the memory collected\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args[\"num_epochs\"]):\n",
    "                self.train(memory)\n",
    "                \n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_supervised_big_model_final_{iteration}.pt\" )\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_supervised_big_model_final_{iteration}.pt\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741a3ce",
   "metadata": {},
   "source": [
    "#### 3.4 Training with Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f04f948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009011268615722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 500,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee177453231a4aad88bfd1e433ec86c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.053261518478393555,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e637aa2866642e589385da93cafa16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 66535.33 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# # model.load_state_dict(torch.load('model_supervised_big_model_1.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# optimizer.load_state_dict(torch.load('optimizer_supervised_big_model_1.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 500,\n",
    "    'num_iterations': 1,\n",
    "    'start_iteration': 0, ### DONT FORGET\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "#     'max_depth':100\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero_Supervised(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory_ = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66902377",
   "metadata": {},
   "source": [
    "#### 3.5 Game between Supervised Self Play and Winner Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "083d9258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game between self_play_agent and competition winner:\n"
     ]
    }
   ],
   "source": [
    "model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_supervised_big_model_final_0.pt', map_location=device))\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':3\n",
    "}\n",
    "mcts = MCTS(model,args, device)\n",
    "agent_supervised_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Game between self_play_agent and competition winner:\")\n",
    "play_game(self_play_agent, self_play_agent, agent1_args=agent_supervised_args, agent2_args = agent_with_heuristics_args)\n",
    "# play_game(self_play_agent, self_play_agent, agent2_args=agent_supervised_args, agent1_args = agent_with_heuristics_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35f88c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game between self_play_agent and greedy:\n",
      "Move number 1:\n",
      "State:\n",
      "[[ 0  0  1 -1  0  0  0  0  0]\n",
      " [ 0  1 -1  1 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0\n",
      "Action played: (0, 2, 0, 3)\n",
      "\n",
      "Move number 2:\n",
      "State:\n",
      "[[ 0  0  0  2  0  0  0  0  0]\n",
      " [ 0  1 -1  1 -1  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (1, 4, 0, 3)\n",
      "\n",
      "Move number 3:\n",
      "State:\n",
      "[[ 0  0  0 -3  0  0  0  0  0]\n",
      " [ 0  1 -1  1  0  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0\n",
      "Action played: (1, 3, 0, 3)\n",
      "\n",
      "Move number 4:\n",
      "State:\n",
      "[[ 0  0  0  4  0  0  0  0  0]\n",
      " [ 0  1 -1  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (1, 2, 0, 3)\n",
      "\n",
      "Move number 5:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0 -1  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (1, 1, 2, 1)\n",
      "\n",
      "Move number 6:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1 -1  1 -1  1  0  0]\n",
      " [ 0  1 -1  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (3, 2, 3, 1)\n",
      "\n",
      "Move number 7:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1 -1  1 -1  1  0  0]\n",
      " [ 0 -2  0  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (2, 1, 3, 1)\n",
      "\n",
      "Move number 8:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 -1  1 -1  1  0  0]\n",
      " [ 0  4  0  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: 0.1\n",
      "Action played: (2, 5, 2, 6)\n",
      "\n",
      "Move number 9:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 -1  1  0 -2  0  0]\n",
      " [ 0  4  0  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (2, 2, 3, 1)\n",
      "\n",
      "Move number 10:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0 -2  0  0]\n",
      " [ 0  5  0  1 -1  1 -1  1 -1]\n",
      " [ 1 -1  1 -1  0 -1  1 -1  1]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (3, 8, 4, 8)\n",
      "\n",
      "Move number 11:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0 -2  0  0]\n",
      " [ 0  5  0  1 -1  1 -1  1  0]\n",
      " [ 1 -1  1 -1  0 -1  1 -1 -2]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (3, 5, 2, 6)\n",
      "\n",
      "Move number 12:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  3  0  0]\n",
      " [ 0  5  0  1 -1  0 -1  1  0]\n",
      " [ 1 -1  1 -1  0 -1  1 -1 -2]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (4, 1, 4, 0)\n",
      "\n",
      "Move number 13:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  3  0  0]\n",
      " [ 0  5  0  1 -1  0 -1  1  0]\n",
      " [-2  0  1 -1  0 -1  1 -1 -2]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (2, 6, 3, 6)\n",
      "\n",
      "Move number 14:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0  4  1  0]\n",
      " [-2  0  1 -1  0 -1  1 -1 -2]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (4, 7, 3, 6)\n",
      "\n",
      "Move number 15:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  1  0]\n",
      " [-2  0  1 -1  0 -1  1  0 -2]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (3, 7, 4, 8)\n",
      "\n",
      "Move number 16:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  1 -1  0 -1  1  0  3]\n",
      " [-1  1 -1  1 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.1\n",
      "Action played: (4, 3, 5, 3)\n",
      "\n",
      "Move number 17:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  1  0  0 -1  1  0  3]\n",
      " [-1  1 -1 -2 -1  1 -1  1  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.2\n",
      "Action played: (4, 8, 5, 7)\n",
      "\n",
      "Move number 18:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  1  0  0 -1  1  0  0]\n",
      " [-1  1 -1 -2 -1  1 -1  4  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (5, 6, 5, 7)\n",
      "\n",
      "Move number 19:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  1  0  0 -1  1  0  0]\n",
      " [-1  1 -1 -2 -1  1  0 -5  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (4, 2, 5, 3)\n",
      "\n",
      "Move number 20:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  0  0  0 -1  1  0  0]\n",
      " [-1  1 -1  3 -1  1  0 -5  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (5, 2, 5, 3)\n",
      "\n",
      "Move number 21:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  0  0  0 -1  1  0  0]\n",
      " [-1  1  0 -4 -1  1  0 -5  0]\n",
      " [ 0  0  1 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (6, 2, 5, 3)\n",
      "\n",
      "Move number 22:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  0  0  0 -1  1  0  0]\n",
      " [-1  1  0  5 -1  1  0 -5  0]\n",
      " [ 0  0  0 -1  1 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (5, 4, 6, 4)\n",
      "\n",
      "Move number 23:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-2  0  0  0  0 -1  1  0  0]\n",
      " [-1  1  0  5  0  1  0 -5  0]\n",
      " [ 0  0  0 -1 -2 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (5, 1, 4, 0)\n",
      "\n",
      "Move number 24:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [ 3  0  0  0  0 -1  1  0  0]\n",
      " [-1  0  0  5  0  1  0 -5  0]\n",
      " [ 0  0  0 -1 -2 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (5, 0, 4, 0)\n",
      "\n",
      "Move number 25:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  1  0 -5  0]\n",
      " [ 0  0  0 -1 -2 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (5, 5, 6, 4)\n",
      "\n",
      "Move number 26:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  3 -1  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action played: (6, 5, 6, 4)\n",
      "\n",
      "Move number 27:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1 -4  0  1 -1  0]\n",
      " [ 0  0  0  0 -1  1 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (7, 5, 6, 4)\n",
      "\n",
      "Move number 28:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  1 -1  0]\n",
      " [ 0  0  0  0 -1  0 -1  1  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (7, 6, 7, 7)\n",
      "\n",
      "Move number 29:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  1 -1  0]\n",
      " [ 0  0  0  0 -1  0  0 -2  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (6, 6, 7, 7)\n",
      "\n",
      "Move number 30:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  3  0]\n",
      " [ 0  0  0  0  0 -1  1  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (8, 5, 8, 6)\n",
      "\n",
      "Move number 31:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  3  0]\n",
      " [ 0  0  0  0  0  0 -2  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (7, 7, 8, 6)\n",
      "\n",
      "Move number 32:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0  1 -1  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (3, 4, 3, 3)\n",
      "\n",
      "Move number 33:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 0  5  0 -2  0  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (2, 4, 3, 3)\n",
      "\n",
      "Move number 34:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  5  0  3  0  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (2, 3, 3, 3)\n",
      "\n",
      "Move number 35:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0 -4  0  0 -5  0  0]\n",
      " [-4  0  0  0  0 -1  1  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Score: -0.4\n",
      "Action played: (4, 6, 4, 5)\n",
      "\n",
      "Move number 36:\n",
      "State:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0 -4  0  0 -5  0  0]\n",
      " [-4  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -1  5  0  0 -1  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Score: -0.3\n",
      "Action played: (7, 4, 6, 3)\n",
      "\n",
      "Final state:\n",
      "[[ 0  0  0 -5  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0 -4  0  0 -5  0  0]\n",
      " [-4  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  5  0  0  0 -5  0]\n",
      " [ 0  0  0 -2  5  0  0 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0]]\n",
      "Final score: -0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Game between self_play_agent and greedy:\")\n",
    "play_game(greedy_agent, self_play_agent, agent2_args=agent_supervised_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e476442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03265810012817383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e81b4ac3ff49b191e6914d496f122d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023255109786987305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cad983acf6a447d83f4f6c88be5f41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023998260498046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b54c24182d4f20a7c8c8cc39ee23f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_games(num_games, agent1, agent2, agent1_args=None, agent2_args=None):\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    for i in trange(num_games):\n",
    "        if i % 2 == 0:\n",
    "            score = play_game(agent1, agent2, agent1_args, agent2_args, display=False)\n",
    "        else:\n",
    "            score = -play_game(agent2, agent1, agent2_args, agent1_args, display=False)\n",
    "\n",
    "        if score > 0:\n",
    "            agent1_wins += 1\n",
    "        elif score < 0:\n",
    "            agent2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "\n",
    "num_games = 4\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_supervised_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_supervised_args)\n",
    "self_play_vs_heuristics_agent = play_games(num_games, self_play_agent, self_play_agent, agent1_args=agent_supervised_args, agent2_args=agent_with_heuristics_args)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, self_play_vs_random, width, label='Self Play with supervised vs Random')\n",
    "rects2 = ax.bar(x, self_play_vs_greedy, width, label='Self Play with supervised vs Greedy')\n",
    "rects3 = ax.bar(x + width, self_play_vs_heuristics_agent, width, label='Self Play with supervised vs Competition winner')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent Performance against Random, Greedy and Competition Winner Agents')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95982f",
   "metadata": {},
   "source": [
    "#### Testing Supervised Self Play with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91ce57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ResNet( 3, 32, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model.load_state_dict(torch.load('model_supervised_12.pt', map_location=device))\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':100\n",
    "}\n",
    "mcts = MCTS(model,args, device)\n",
    "agent_supervised_data_augm_args = {'model': model, 'mcts': mcts, 'args': args}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Game between self_play_agent and random_agent:\")\n",
    "play_game(self_play_agent, self_play_agent, agent1_args=agent_supervised_data_augm_args, agent2_args = agent_supervised_args)\n",
    "# play_game(greedy_agent, self_play_agent, agent2_args=agent_supervised_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 4\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_supervised_data_augm_args)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_supervised_data_augm_args)\n",
    "self_play_vs_heuristics_agent = play_games(num_games, self_play_agent, self_play_agent, agent1_args=agent_supervised_data_augm_args, agent2_args=agent_with_heuristics_args)\n",
    "self_play_vs_supervised_wo_data_augmentation = play_games(num_games, self_play_agent, self_play_agent, agent1_args=agent_supervised_data_augm_args, agent2_args=agent_supervised_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - 3*width/2, self_play_vs_random, width, label='Data Augmentation vs Random')\n",
    "rects2 = ax.bar(x - width/2, self_play_vs_greedy, width, label='Data Augmentation vs Greedy')\n",
    "rects3 = ax.bar(x + width/2, self_play_vs_heuristics_agent, width, label='Data Augmentation vs Competition Winner')\n",
    "rects4 = ax.bar(x + 3*width/2, self_play_vs_supervised_wo_data_augmentation, width, label='Data Augmentation vs No Data Augmentation')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent with Supervised and Data Augmentation Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f790e7c",
   "metadata": {},
   "source": [
    "### Increase Model Architecture Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a39005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "# model.load_state_dict(torch.load('model_1.pt', map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# optimizer.load_state_dict(torch.load('optimizer_1.pt', map_location=device))\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 300,\n",
    "    'num_iterations': 3,\n",
    "    'start_iteration': 10,\n",
    "    'num_selfPlay_iterations': 100,\n",
    "    'num_epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "#     'max_depth':100\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero_Supervised(model, optimizer, args)\n",
    "\n",
    "start_time = time.time()\n",
    "memory_ = alphaZero.learn()\n",
    "end_time = time.time()\n",
    "\n",
    "time_difference = end_time - start_time\n",
    "print(f'The code took {time_difference:.2f} seconds to run.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d116ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big_model = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model_big_model.load_state_dict(torch.load('model_supervised_bigger_model_2.pt', map_location=device))\n",
    "\n",
    "\n",
    "args = {\n",
    "    'C': 1.25,\n",
    "    'num_searches': 500,\n",
    "    'action_size': actions_size,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3, \n",
    "    'max_depth':100\n",
    "}\n",
    "mcts_big_model = MCTS(model_big_model,args, device)\n",
    "agent_supervised_bigger_model = {'model': model_big_model, 'mcts': mcts_big_model, 'args': args}\n",
    "\n",
    "## model with data augmentation\n",
    "model_big_model_dt_aug = ResNet( 6, 64, device=device, board_size = board_size, actions_size = actions_size)\n",
    "model_big_model_dt_aug.load_state_dict(torch.load('model_supervised_bigger_model_12.pt', map_location=device))\n",
    "\n",
    "mcts_big_model_dt_aug = MCTS(model_big_model_dt_aug,args, device)\n",
    "agent_supervised_bigger_model_dt_aug = {'model': model_big_model_dt_aug, 'mcts': mcts_big_model_dt_aug, 'args': args}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Game between self_play_agent and random_agent:\")\n",
    "play_game(greedy_agent, self_play_agent, agent1_args=None, agent2_args = agent_supervised_args)\n",
    "# play_game(greedy_agent, self_play_agent, agent2_args=agent_supervised_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 4\n",
    "\n",
    "self_play_vs_random = play_games(num_games, self_play_agent, random_agent, agent1_args=agent_supervised_bigger_model)\n",
    "self_play_vs_greedy = play_games(num_games, self_play_agent, greedy_agent, agent1_args=agent_supervised_bigger_model)\n",
    "self_play_vs_heuristics_agent = play_games(num_games, self_play_agent, self_play_agent, agent1_args=agent_supervised_bigger_model, agent2_args=agent_with_heuristics_args)\n",
    "self_play_vs_supervised_wo_data_augmentation = play_games(num_games, self_play_agent, self_play_agent, agent1_args=agent_supervised_bigger_model, agent2_args=agent_supervised_args)\n",
    "\n",
    "# Plotting results\n",
    "x = np.arange(3)\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - 3*width/2, self_play_vs_random, width, label='Bigger Model vs Random')\n",
    "rects2 = ax.bar(x - width/2, self_play_vs_greedy, width, label='Bigger Model vs Greedy')\n",
    "rects3 = ax.bar(x + width/2, self_play_vs_heuristics_agent, width, label='Bigger Model vs Competition Winner')\n",
    "rects4 = ax.bar(x + 3*width/2, self_play_vs_supervised_wo_data_augmentation, width, label='Bigger Model vs Non Bigger Model')\n",
    "\n",
    "ax.set_ylabel('Number of Games')\n",
    "ax.set_title('Self Play Agent with Supervised and Data Augmentation Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Wins', 'Losses', 'Draws'])\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
